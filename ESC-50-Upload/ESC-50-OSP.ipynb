{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ebaef4f-2472-402d-951e-8748ffd2df47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>fold</th>\n",
       "      <th>target</th>\n",
       "      <th>category</th>\n",
       "      <th>esc10</th>\n",
       "      <th>src_file</th>\n",
       "      <th>take</th>\n",
       "      <th>filepath</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1-100032-A-0.wav</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>dog</td>\n",
       "      <td>True</td>\n",
       "      <td>100032</td>\n",
       "      <td>A</td>\n",
       "      <td>ESC-50-master/audio/1-100032-A-0.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1-100038-A-14.wav</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>chirping_birds</td>\n",
       "      <td>False</td>\n",
       "      <td>100038</td>\n",
       "      <td>A</td>\n",
       "      <td>ESC-50-master/audio/1-100038-A-14.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1-100210-A-36.wav</td>\n",
       "      <td>1</td>\n",
       "      <td>36</td>\n",
       "      <td>vacuum_cleaner</td>\n",
       "      <td>False</td>\n",
       "      <td>100210</td>\n",
       "      <td>A</td>\n",
       "      <td>ESC-50-master/audio/1-100210-A-36.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1-100210-B-36.wav</td>\n",
       "      <td>1</td>\n",
       "      <td>36</td>\n",
       "      <td>vacuum_cleaner</td>\n",
       "      <td>False</td>\n",
       "      <td>100210</td>\n",
       "      <td>B</td>\n",
       "      <td>ESC-50-master/audio/1-100210-B-36.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1-101296-A-19.wav</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>thunderstorm</td>\n",
       "      <td>False</td>\n",
       "      <td>101296</td>\n",
       "      <td>A</td>\n",
       "      <td>ESC-50-master/audio/1-101296-A-19.wav</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            filename  fold  target        category  esc10  src_file take  \\\n",
       "0   1-100032-A-0.wav     1       0             dog   True    100032    A   \n",
       "1  1-100038-A-14.wav     1      14  chirping_birds  False    100038    A   \n",
       "2  1-100210-A-36.wav     1      36  vacuum_cleaner  False    100210    A   \n",
       "3  1-100210-B-36.wav     1      36  vacuum_cleaner  False    100210    B   \n",
       "4  1-101296-A-19.wav     1      19    thunderstorm  False    101296    A   \n",
       "\n",
       "                                filepath  \n",
       "0   ESC-50-master/audio/1-100032-A-0.wav  \n",
       "1  ESC-50-master/audio/1-100038-A-14.wav  \n",
       "2  ESC-50-master/audio/1-100210-A-36.wav  \n",
       "3  ESC-50-master/audio/1-100210-B-36.wav  \n",
       "4  ESC-50-master/audio/1-101296-A-19.wav  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Load CSV metadata\n",
    "meta_df = pd.read_csv('ESC-50-master/meta/esc50.csv')\n",
    "\n",
    "# Add full path to audio files\n",
    "meta_df['filepath'] = meta_df['filename'].apply(lambda x: os.path.join('ESC-50-master/audio/', x))\n",
    "\n",
    "# Display sample\n",
    "meta_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3987d4d-ce9c-4171-95f8-268b1c93aca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import librosa\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from audiomentations import Compose, AddGaussianNoise, PitchShift, TimeStretch, Gain\n",
    "\n",
    "class ESC50Dataset(Dataset):\n",
    "    def __init__(self, df, sample_rate=44100, duration=5.0, augment_type='none', n_mels=128):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.sr = sample_rate\n",
    "        self.length = int(sample_rate * duration)\n",
    "        self.augment_type = augment_type\n",
    "        self.n_mels = n_mels\n",
    "\n",
    "        self.weak_transform = Compose([\n",
    "            AddGaussianNoise(min_amplitude=0.001, max_amplitude=0.015, p=0.5),\n",
    "        ])\n",
    "        self.strong_transform = Compose([\n",
    "            AddGaussianNoise(min_amplitude=0.001, max_amplitude=0.02, p=0.5),\n",
    "            PitchShift(min_semitones=-4, max_semitones=4, p=0.5),\n",
    "            TimeStretch(min_rate=0.8, max_rate=1.25, p=0.5),\n",
    "            Gain(min_gain_db=-6, max_gain_db=6, p=0.5)\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        path = row['filepath']\n",
    "        label = row['target'] if 'target' in row else -1\n",
    "\n",
    "        y, _ = librosa.load(path, sr=self.sr)\n",
    "        if len(y) < self.length:\n",
    "            y = np.pad(y, (0, self.length - len(y)))\n",
    "        else:\n",
    "            y = y[:self.length]\n",
    "\n",
    "        if self.augment_type == 'weak':\n",
    "            y = self.weak_transform(samples=y, sample_rate=self.sr)\n",
    "        elif self.augment_type == 'strong':\n",
    "            y = self.strong_transform(samples=y, sample_rate=self.sr)\n",
    "\n",
    "        # Convert to Mel spectrogram\n",
    "        mel = librosa.feature.melspectrogram(y=y, sr=self.sr, n_mels=self.n_mels)\n",
    "        mel_db = librosa.power_to_db(mel + 1e-6, ref=np.max)\n",
    "        mel_db = np.clip(mel_db, a_min=-80, a_max=0)  # Avoid crazy ranges\n",
    "\n",
    "\n",
    "        # Normalize and convert to torch tensor [1, H, W]\n",
    "        mel_tensor = torch.tensor(mel_db, dtype=torch.float32).unsqueeze(0)\n",
    "\n",
    "        return mel_tensor, torch.tensor(label, dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "00fb37ab-74a8-4618-a9b3-75159b10d4dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_ssl_with_ood(id_df, ood_df, labeled_fraction=0.2, ood_fraction=0.3, save_dir=\"osp-split\"):\n",
    "    import os\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    labeled_df_list = []\n",
    "    unlabeled_combined_list = []\n",
    "\n",
    "    # Process ID classes\n",
    "    for label in sorted(id_df['target'].unique()):\n",
    "        class_df = id_df[id_df['target'] == label]\n",
    "        n_labeled = max(1, int(len(class_df) * labeled_fraction))\n",
    "\n",
    "        # Labeled samples\n",
    "        labeled_samples = class_df.sample(n=n_labeled, random_state=42)\n",
    "        labeled_df_list.append(labeled_samples)\n",
    "\n",
    "        # Remaining go to unlabeled (with is_ood = 0), drop target/category\n",
    "        unlabeled_samples = class_df.drop(labeled_samples.index).copy()\n",
    "        unlabeled_samples[\"is_ood\"] = 0\n",
    "        unlabeled_samples = unlabeled_samples.drop(columns=[\"target\", \"category\"])\n",
    "        unlabeled_combined_list.append(unlabeled_samples)\n",
    "\n",
    "    # Combine labeled ID samples\n",
    "    labeled_df = pd.concat(labeled_df_list).reset_index(drop=True)\n",
    "\n",
    "    # Combine ID-unlabeled samples first\n",
    "    id_unlabeled_df = pd.concat(unlabeled_combined_list).reset_index(drop=True)\n",
    "    \n",
    "    # n_ood = γ / (1 - γ) * ID-unlabeled\n",
    "    n_id_unlabeled = len(id_unlabeled_df)\n",
    "    n_ood = int(ood_fraction * n_id_unlabeled / (1 - ood_fraction))\n",
    "    \n",
    "    # Safe sampling: don't go beyond available OOD data\n",
    "    n_ood = min(n_ood, len(ood_df))\n",
    "    \n",
    "    # Sample OOD data\n",
    "    unlabeled_ood_df = ood_df.sample(n=n_ood, random_state=42).copy().reset_index(drop=True)\n",
    "    unlabeled_ood_df[\"is_ood\"] = 1\n",
    "    unlabeled_ood_df = unlabeled_ood_df.drop(columns=[\"target\", \"category\"])\n",
    "    \n",
    "    # Combine and save\n",
    "    unlabeled_df = pd.concat([id_unlabeled_df, unlabeled_ood_df], ignore_index=True)\n",
    "\n",
    "    # Save files\n",
    "    labeled_df.to_csv(f\"{save_dir}/labeled{ood_fraction}.csv\", index=False)\n",
    "    unlabeled_df.to_csv(f\"{save_dir}/unlabeled{ood_fraction}.csv\", index=False)\n",
    "\n",
    "    print(f\"Saved {len(labeled_df)} labeled samples and {len(unlabeled_df)} unlabeled samples to '{save_dir}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "66d9c1ff-5b55-4656-8580-c0de1b1af93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def get_ssl_loaders(meta_df,labeled_fraction=0.1, fold=1, batch_size=16, split_dir=\"labeled-unlabeled\"):\n",
    "    # Load pre-saved labeled and unlabeled CSVs for this fold\n",
    "    labeled_df = pd.read_csv(f\"{split_dir}/{labeled_fraction}/labeled.csv\")\n",
    "    unlabeled_df = pd.read_csv(f\"{split_dir}/{labeled_fraction}/unlabeled.csv\")\n",
    "\n",
    "    # Get validation set from meta_df\n",
    "    val_df = meta_df[meta_df['fold'] == fold]\n",
    "\n",
    "    # Create datasets\n",
    "    labeled_dataset = ESC50Dataset(labeled_df, augment_type='weak')\n",
    "    unlabeled_dataset = DualViewESC50Dataset(unlabeled_df)\n",
    "    val_dataset = ESC50Dataset(val_df, augment_type='none')\n",
    "\n",
    "    # Create loaders\n",
    "    labeled_loader = DataLoader(labeled_dataset, batch_size=batch_size, shuffle=True)\n",
    "    unlabeled_loader = DataLoader(unlabeled_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "    return labeled_loader, unlabeled_loader, val_loader\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "053914b5-6b96-4e5b-8810-75d0b8060f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DualViewESC50Dataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.weak_dataset = ESC50Dataset(df, augment_type='weak')\n",
    "        self.strong_dataset = ESC50Dataset(df, augment_type='strong')\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.weak_dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        weak_x, _ = self.weak_dataset[idx]\n",
    "        strong_x, _ = self.strong_dataset[idx]\n",
    "        return weak_x, strong_x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b5a22d37-9356-4a02-826b-e46dab9abeeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 0, 1: 1, 2: 2, 3: 3, 5: 4, 6: 5, 7: 6, 8: 7, 10: 8, 12: 9, 13: 10, 14: 11, 15: 12, 17: 13, 18: 14, 20: 15, 21: 16, 22: 17, 24: 18, 27: 19, 28: 20, 30: 21, 31: 22, 32: 23, 33: 24, 34: 25, 35: 26, 36: 27, 37: 28, 38: 29, 39: 30, 40: 31, 41: 32, 42: 33, 43: 34, 44: 35, 46: 36, 47: 37, 48: 38, 49: 39}\n",
      "ID Classes: [0, 1, 2, 3, 5, 6, 7, 8, 10, 12, 13, 14, 15, 17, 18, 20, 21, 22, 24, 27, 28, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 46, 47, 48, 49]\n",
      "OOD Classes: [4, 9, 11, 16, 19, 23, 25, 26, 29, 45]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "random.seed(42)\n",
    "\n",
    "# All 50 class indices\n",
    "all_class_ids = list(range(50))\n",
    "\n",
    "# Randomly choose ID and OOD classes\n",
    "num_id_classes = 40\n",
    "id_class_ids = sorted(random.sample(all_class_ids, num_id_classes))\n",
    "label_map = {orig_class: new_idx for new_idx, orig_class in enumerate(sorted(id_class_ids))}\n",
    "print(label_map)\n",
    "\n",
    "ood_class_ids = sorted(list(set(all_class_ids) - set(id_class_ids)))\n",
    "\n",
    "print(f\"ID Classes: {id_class_ids}\")\n",
    "print(f\"OOD Classes: {ood_class_ids}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eca7fc50-2d34-4c06-96c1-967473d1f692",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600\n",
      "            filename  fold  target         category  esc10  src_file take  \\\n",
      "0   1-100032-A-0.wav     1       0              dog   True    100032    A   \n",
      "1  1-100038-A-14.wav     1      14   chirping_birds  False    100038    A   \n",
      "2  1-100210-A-36.wav     1      36   vacuum_cleaner  False    100210    A   \n",
      "3  1-100210-B-36.wav     1      36   vacuum_cleaner  False    100210    B   \n",
      "6  1-101336-A-30.wav     1      30  door_wood_knock  False    101336    A   \n",
      "\n",
      "                                filepath  \n",
      "0   ESC-50-master/audio/1-100032-A-0.wav  \n",
      "1  ESC-50-master/audio/1-100038-A-14.wav  \n",
      "2  ESC-50-master/audio/1-100210-A-36.wav  \n",
      "3  ESC-50-master/audio/1-100210-B-36.wav  \n",
      "6  ESC-50-master/audio/1-101336-A-30.wav  \n",
      "400\n",
      "             filename  fold  target      category  esc10  src_file take  \\\n",
      "4   1-101296-A-19.wav     1      19  thunderstorm  False    101296    A   \n",
      "5   1-101296-B-19.wav     1      19  thunderstorm  False    101296    B   \n",
      "8    1-103298-A-9.wav     1       9          crow  False    103298    A   \n",
      "16  1-115521-A-19.wav     1      19  thunderstorm  False    115521    A   \n",
      "28  1-119125-A-45.wav     1      45         train  False    119125    A   \n",
      "\n",
      "                                 filepath  \n",
      "4   ESC-50-master/audio/1-101296-A-19.wav  \n",
      "5   ESC-50-master/audio/1-101296-B-19.wav  \n",
      "8    ESC-50-master/audio/1-103298-A-9.wav  \n",
      "16  ESC-50-master/audio/1-115521-A-19.wav  \n",
      "28  ESC-50-master/audio/1-119125-A-45.wav  \n"
     ]
    }
   ],
   "source": [
    "id_df = meta_df[meta_df['target'].isin(id_class_ids)].copy()\n",
    "ood_df = meta_df[meta_df['target'].isin(ood_class_ids)].copy()\n",
    "\n",
    "print(len(id_df))\n",
    "print(id_df.head())\n",
    "print(len(ood_df))\n",
    "print(ood_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5dbf23db-60eb-4a71-a979-afc9093ad592",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 320 labeled samples and 1680 unlabeled samples to 'osp-split/0.2'\n"
     ]
    }
   ],
   "source": [
    "split_ssl_with_ood(\n",
    "    id_df=id_df,\n",
    "    ood_df=ood_df,\n",
    "    labeled_fraction=0.2,   \n",
    "    ood_fraction=0.3,       # this is γ in the OSP paper\n",
    "    save_dir=\"osp-split/0.2\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1dd9817c-ca26-4018-bd2c-e11c34a69d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class OSPModel(nn.Module):\n",
    "    def __init__(self, num_classes=40, backbone='resnet18'):\n",
    "        super(OSPModel, self).__init__()\n",
    "        self.encoder = self._build_encoder(backbone)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            dummy = torch.randn(1, 1, 128, 431)\n",
    "            feature_size = self.encoder(dummy).shape[1]\n",
    "\n",
    "        self.classifier_head = nn.Linear(feature_size, num_classes)\n",
    "        self.ssp_head = nn.Linear(feature_size, 4)  # 4 audio transforms\n",
    "\n",
    "    def _build_encoder(self, backbone):\n",
    "        model = models.resnet18(pretrained=True)\n",
    "        model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        model.fc = nn.Identity()\n",
    "        return model\n",
    "\n",
    "    def forward(self, x, task='class'):\n",
    "        features = self.encoder(x)\n",
    "        features = features.view(features.size(0), -1)\n",
    "\n",
    "        if task == 'class':\n",
    "            return self.classifier_head(features)\n",
    "        elif task == 'ssp':\n",
    "            return self.ssp_head(features)\n",
    "        elif task == 'feature':\n",
    "            return features\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown task '{task}'\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c00d5cde-95a5-485b-a952-6cf1688aeeff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from audiomentations import Compose, TimeStretch, PitchShift, Gain\n",
    "\n",
    "class AudioTransformDataset(Dataset):\n",
    "    def __init__(self, df, sample_rate=44100, duration=5.0, n_mels=128):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.sr = sample_rate\n",
    "        self.length = int(sample_rate * duration)\n",
    "        self.n_mels = n_mels\n",
    "\n",
    "        self.transforms = [\n",
    "            lambda x: x,  # No transform → class 0\n",
    "            TimeStretch(min_rate=0.8, max_rate=0.8, p=1.0),  # class 1\n",
    "            PitchShift(min_semitones=4, max_semitones=4, p=1.0),  # class 2\n",
    "            Gain(min_gain_db=6, max_gain_db=6, p=1.0),  # class 3\n",
    "        ]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        path = row['filepath']\n",
    "\n",
    "        y, _ = librosa.load(path, sr=self.sr)\n",
    "        if len(y) < self.length:\n",
    "            y = np.pad(y, (0, self.length - len(y)))\n",
    "        else:\n",
    "            y = y[:self.length]\n",
    "\n",
    "        # Apply random transformation\n",
    "        label = random.randint(0, 3)\n",
    "        y_aug = self.transforms[label](samples=y, sample_rate=self.sr) if label != 0 else y\n",
    "\n",
    "        mel = librosa.feature.melspectrogram(y=y_aug, sr=self.sr, n_mels=self.n_mels)\n",
    "        mel_db = librosa.power_to_db(mel + 1e-6, ref=np.max)\n",
    "        mel_db = np.clip(mel_db, a_min=-80, a_max=0)\n",
    "        mel_tensor = torch.tensor(mel_db, dtype=torch.float32).unsqueeze(0)\n",
    "\n",
    "        return mel_tensor, torch.tensor(label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "93df6e9c-6477-4ff4-8f27-e3a19d29db23",
   "metadata": {},
   "outputs": [],
   "source": [
    "unlabeled_df = pd.read_csv(\"osp-split/0.2/unlabeled0.3.csv\")\n",
    "ssp_dataset = AudioTransformDataset(unlabeled_df)\n",
    "ssp_loader = DataLoader(ssp_dataset, batch_size=32, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6a70b204-e964-4143-b8af-6ce08d270a8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Arnav\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Arnav\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "model = OSPModel(num_classes=num_id_classes)\n",
    "\n",
    "# Losses\n",
    "criterion_cls = nn.CrossEntropyLoss()\n",
    "criterion_rot = nn.CrossEntropyLoss()\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=3e-4)\n",
    "\n",
    "def train_osp(model, labeled_loader, ssp_loader, num_epochs=50, device='cuda', save_path=\"osp_pretrained.pt\"):\n",
    "    model.to(device)\n",
    "\n",
    "    # Track current epoch if resuming\n",
    "    start_epoch = 0\n",
    "\n",
    "    # Load pretrained checkpoint if it exists\n",
    "    if os.path.exists(save_path):\n",
    "        print(f\"Loading checkpoint from: {save_path}\")\n",
    "        checkpoint = torch.load(save_path, map_location=device)\n",
    "        model.load_state_dict(checkpoint['model_state'])\n",
    "        start_epoch = checkpoint.get('epoch', 0)\n",
    "        print(f\"Resuming training from epoch {start_epoch + 1}\")\n",
    "    else:\n",
    "        print(\"Starting pretraining from scratch...\")\n",
    "\n",
    "    model.train()\n",
    "    criterion_cls = nn.CrossEntropyLoss()\n",
    "    criterion_ssp = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=3e-4)\n",
    "\n",
    "    for epoch in range(start_epoch, num_epochs):\n",
    "        total_cls_loss = 0.0\n",
    "        total_ssp_loss = 0.0\n",
    "        correct_cls = 0\n",
    "        total_cls = 0\n",
    "        correct_ssp = 0\n",
    "        total_ssp = 0\n",
    "\n",
    "        # Supervised classification\n",
    "        for x, y in labeled_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "\n",
    "            pred = model(x, task='class')\n",
    "            loss_cls = criterion_cls(pred, y)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss_cls.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_cls_loss += loss_cls.item()\n",
    "            preds_cls = pred.argmax(dim=1)\n",
    "            correct_cls += (preds_cls == y).sum().item()\n",
    "            total_cls += y.size(0)\n",
    "\n",
    "        # Self-supervised transform prediction\n",
    "        for x_aug, ssp_label in ssp_loader:\n",
    "            x_aug, ssp_label = x_aug.to(device), ssp_label.to(device)\n",
    "\n",
    "            pred_ssp = model(x_aug, task='ssp')\n",
    "            loss_ssp = criterion_ssp(pred_ssp, ssp_label)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss_ssp.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_ssp_loss += loss_ssp.item()\n",
    "            preds_ssp = pred_ssp.argmax(dim=1)\n",
    "            correct_ssp += (preds_ssp == ssp_label).sum().item()\n",
    "            total_ssp += ssp_label.size(0)\n",
    "\n",
    "        cls_acc = 100 * correct_cls / total_cls if total_cls else 0\n",
    "        ssp_acc = 100 * correct_ssp / total_ssp if total_ssp else 0\n",
    "\n",
    "        print(f\"[Epoch {epoch+1}] \"\n",
    "              f\"Cls Loss: {total_cls_loss:.4f} | Cls Acc: {cls_acc:.2f}% || \"\n",
    "              f\"SSP Loss: {total_ssp_loss:.4f} | SSP Acc: {ssp_acc:.2f}%\")\n",
    "\n",
    "        # Save checkpoint after every epoch\n",
    "        torch.save({\n",
    "            'epoch': epoch + 1,\n",
    "            'model_state': model.state_dict(),\n",
    "        }, save_path)\n",
    "        print(f\"Checkpoint saved to {save_path}\")\n",
    "\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f084ca79-9c9f-416d-aa24-cf5512e61a60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint from: models/ESC-50/OSP/osp_pretrained.pt\n",
      "Resuming training from epoch 13\n",
      "[Epoch 13] Cls Loss: 6.5420 | Cls Acc: 90.62% || SSP Loss: 34.3659 | SSP Acc: 66.13%\n",
      "Checkpoint saved to models/ESC-50/OSP/osp_pretrained.pt\n",
      "[Epoch 14] Cls Loss: 6.0413 | Cls Acc: 94.06% || SSP Loss: 33.9267 | SSP Acc: 66.01%\n",
      "Checkpoint saved to models/ESC-50/OSP/osp_pretrained.pt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 15\u001b[0m\n\u001b[0;32m     12\u001b[0m labeled_loader \u001b[38;5;241m=\u001b[39m DataLoader(labeled_dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     13\u001b[0m ssp_loader \u001b[38;5;241m=\u001b[39m DataLoader(ssp_dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m---> 15\u001b[0m trained_model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_osp\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabeled_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabeled_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mssp_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mssp_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m15\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_available\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodels/ESC-50/OSP/osp_pretrained.pt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[0;32m     22\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[11], line 67\u001b[0m, in \u001b[0;36mtrain_osp\u001b[1;34m(model, labeled_loader, ssp_loader, num_epochs, device, save_path)\u001b[0m\n\u001b[0;32m     64\u001b[0m loss_ssp \u001b[38;5;241m=\u001b[39m criterion_ssp(pred_ssp, ssp_label)\n\u001b[0;32m     66\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 67\u001b[0m \u001b[43mloss_ssp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     68\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     70\u001b[0m total_ssp_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss_ssp\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\_tensor.py:626\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    618\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    619\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    624\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    625\u001b[0m     )\n\u001b[1;32m--> 626\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    627\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    628\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\autograd\\graph.py:823\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    821\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    822\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    824\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    825\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    826\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    827\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "os.makedirs(\"models/ESC-50/OSP\", exist_ok=True)\n",
    "\n",
    "labeled_df = pd.read_csv(\"osp-split/0.2/labeled0.3.csv\")\n",
    "labeled_df['target'] = labeled_df['target'].map(label_map)\n",
    "\n",
    "unlabeled_df = pd.read_csv(\"osp-split/0.2/unlabeled0.3.csv\")\n",
    "\n",
    "# Create datasets\n",
    "labeled_dataset = ESC50Dataset(labeled_df, augment_type='weak')\n",
    "ssp_dataset = AudioTransformDataset(unlabeled_df)  # for SSP pretraining\n",
    "\n",
    "labeled_loader = DataLoader(labeled_dataset, batch_size=16, shuffle=True)\n",
    "ssp_loader = DataLoader(ssp_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "trained_model = train_osp(\n",
    "    model=model,\n",
    "    labeled_loader=labeled_loader,\n",
    "    ssp_loader=ssp_loader,\n",
    "    num_epochs=15,\n",
    "    device='cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    save_path=\"models/ESC-50/OSP/osp_pretrained.pt\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7c5bdd15-dffe-49c1-8c57-1abb601f06bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40\n",
      "Pretrained Model loaded.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OSPModel(\n",
       "  (encoder): ResNet(\n",
       "    (conv1): Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (layer1): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer3): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer4): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "    (fc): Identity()\n",
       "  )\n",
       "  (classifier_head): Linear(in_features=512, out_features=40, bias=True)\n",
       "  (ssp_head): Linear(in_features=512, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(label_map))\n",
    "model = OSPModel(num_classes=len(label_map))\n",
    "checkpoint = torch.load(\"models/ESC-50/OSP/osp_pretrained.pt\", map_location='cpu')\n",
    "model.load_state_dict(checkpoint['model_state'])\n",
    "model.eval()\n",
    "print(\"Pretrained Model loaded.\")\n",
    "model.to('cuda' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "635c0635-4c4b-4d6b-9bb9-a2431516161f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureExtractorDataset(Dataset):\n",
    "    def __init__(self, df, device='cpu'):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.device = device\n",
    "        self.dataset = ESC50Dataset(df, augment_type='none')\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x, _ = self.dataset[idx]\n",
    "        is_ood = self.df.iloc[idx].get(\"is_ood\", -1)\n",
    "        return x.to(self.device), is_ood\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "43539d7b-b675-4373-bfa8-eddcd018b228",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "unlabeled_df = pd.read_csv(\"osp-split/0.2/unlabeled0.3.csv\")\n",
    "feature_dataset = FeatureExtractorDataset(unlabeled_df, device='cuda' if torch.cuda.is_available() else 'cpu')\n",
    "feature_loader = DataLoader(feature_dataset, batch_size=32)\n",
    "\n",
    "features = []\n",
    "is_ood_flags = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x, flag in feature_loader:\n",
    "        feats = model(x, task='feature')\n",
    "        features.append(feats.cpu())\n",
    "        is_ood_flags.append(flag.cpu())\n",
    "\n",
    "features = torch.cat(features)  # shape: [N, D]\n",
    "is_ood_flags = torch.cat(is_ood_flags)  # shape: [N]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1ed4750d-9558-42e5-83ac-6d8929a2159b",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_scores = features.norm(dim=1)  # [N] — L2 norm of each feature\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "013ec8bc-af66-4f9e-976e-7e1039d7ff50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist(feature_scores.numpy(), bins=100)\n",
    "plt.title(\"Feature Norms\")\n",
    "plt.xlabel(\"Norm\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a9db7d64-7dcb-492e-8340-3c3f793bbeab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Otsu Threshold: 29.0321\n"
     ]
    }
   ],
   "source": [
    "from skimage.filters import threshold_otsu\n",
    "\n",
    "threshold = threshold_otsu(feature_scores.numpy())\n",
    "print(f\"Otsu Threshold: {threshold:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a84432c0-d219-45a7-b4c5-e6342fadf31e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted ID: 793 / 1680\n",
      "Predicted OOD: 887 / 1680\n"
     ]
    }
   ],
   "source": [
    "id_mask = feature_scores >= threshold  # confident ID\n",
    "ood_mask = feature_scores < threshold  # predicted OOD\n",
    "\n",
    "# sanity check\n",
    "print(f\"Predicted ID: {id_mask.sum().item()} / {len(id_mask)}\")\n",
    "print(f\"Predicted OOD: {ood_mask.sum().item()} / {len(ood_mask)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2fee4dc9-b126-4a5e-aa3f-d4cfeb1e3d29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUROC for OOD detection: 0.4453\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "auroc = roc_auc_score(is_ood_flags.numpy(), -feature_scores.numpy())  # lower norm → more likely OOD\n",
    "print(f\"AUROC for OOD detection: {auroc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "245c0470-0154-4537-95e7-7e8d5d3cea6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = []\n",
    "    for x in feature_loader:\n",
    "        x = x[0].to(device)  # if flag removed\n",
    "        logit = model(x, task='class')\n",
    "        logits.append(logit.cpu())\n",
    "\n",
    "logits = torch.cat(logits)\n",
    "probs = torch.softmax(logits, dim=1)\n",
    "entropy = -torch.sum(probs * torch.log(probs + 1e-8), dim=1)  # shape: [N]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c233bc0d-b9be-494d-8ac0-877770a9f8df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted ID: 677 / 1680\n",
      "Predicted OOD: 1003 / 1680\n",
      "AUROC (entropy): 0.5356\n"
     ]
    }
   ],
   "source": [
    "from skimage.filters import threshold_otsu\n",
    "threshold = threshold_otsu(entropy.numpy())\n",
    "\n",
    "id_mask = entropy <= threshold\n",
    "ood_mask = entropy > threshold\n",
    "\n",
    "print(f\"Predicted ID: {id_mask.sum().item()} / {len(id_mask)}\")\n",
    "print(f\"Predicted OOD: {ood_mask.sum().item()} / {len(ood_mask)}\")\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "auroc = roc_auc_score(is_ood_flags.numpy(), entropy.numpy())\n",
    "print(f\"AUROC (entropy): {auroc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "34a8418f-e579-4487-bd09-21161cdbb7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "\n",
    "# === 1. Build centroids from labeled data ===\n",
    "def compute_class_centroids(model, labeled_loader, device):\n",
    "    model.eval()\n",
    "    class_features = {}\n",
    "    with torch.no_grad():\n",
    "        for x, y in labeled_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            feats = model(x, task='feature')\n",
    "            for f, label in zip(feats, y):\n",
    "                label = label.item()\n",
    "                if label not in class_features:\n",
    "                    class_features[label] = []\n",
    "                class_features[label].append(f.cpu())\n",
    "    centroids = {c: torch.stack(v).mean(dim=0) for c, v in class_features.items()}\n",
    "    return centroids\n",
    "\n",
    "\n",
    "# === 2. Build OOD memory bank ===\n",
    "def build_ood_memory_bank(centroids, ood_features):\n",
    "    memory_bank = {c: [] for c in centroids}\n",
    "    for f_ood in ood_features:\n",
    "        best_c = max(centroids, key=lambda c: F.cosine_similarity(\n",
    "            f_ood.unsqueeze(0), centroids[c].unsqueeze(0)).item())\n",
    "        memory_bank[best_c].append(f_ood)\n",
    "    return memory_bank\n",
    "\n",
    "\n",
    "# === 3. Apply semantic orthogonal removal ===\n",
    "def apply_sor(f, f_ood, alpha=1.0):\n",
    "    proj = (f @ f_ood) / (f_ood.norm()**2 + 1e-8) * f_ood\n",
    "    return f - alpha * proj\n",
    "\n",
    "def apply_aom(f, f_ood_list, alpha=1.0, top_k=3):\n",
    "    # Compute cosine similarities\n",
    "    sims = [F.cosine_similarity(f.unsqueeze(0), v.unsqueeze(0), dim=-1).item() for v in f_ood_list]\n",
    "    top_indices = sorted(range(len(sims)), key=lambda i: sims[i], reverse=True)[:top_k]\n",
    "\n",
    "    total_proj = torch.zeros_like(f)\n",
    "    for i in top_indices:\n",
    "        v = f_ood_list[i]\n",
    "        proj = (f @ v) / (v.norm()**2 + 1e-8) * v\n",
    "        total_proj += proj\n",
    "\n",
    "    return f - alpha * total_proj\n",
    "\n",
    "\n",
    "# === 4. ODC loss (labeled only or unlabeled too) ===\n",
    "def odc_loss(model, feats, labels, memory_bank, alpha=1.0, method='sor', top_k=3):\n",
    "    loss = 0.0\n",
    "    for i in range(feats.size(0)):\n",
    "        c = labels[i].item()\n",
    "        if memory_bank[c]:\n",
    "            f = feats[i]\n",
    "            f_ood_list = [v.to(feats.device) for v in memory_bank[c]]\n",
    "\n",
    "            if method == 'sor':\n",
    "                f_pruned = apply_sor(f, random.choice(f_ood_list), alpha)\n",
    "            elif method == 'aom':\n",
    "                f_pruned = apply_aom(f, f_ood_list, alpha=alpha, top_k=top_k)\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown pruning method: {method}\")\n",
    "\n",
    "            p1 = F.softmax(model.classifier_head(f), dim=-1)\n",
    "            p2 = F.softmax(model.classifier_head(f_pruned), dim=-1)\n",
    "            loss += F.mse_loss(p1, p2, reduction='sum')\n",
    "    return loss / feats.size(0)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0990b3ee-4b11-4c59-86af-1de4fe1c9a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_odc(\n",
    "    model,\n",
    "    labeled_df,\n",
    "    memory_bank,\n",
    "    batch_size=32,\n",
    "    alpha=1.0,\n",
    "    lambda_odc=0.5,\n",
    "    num_epochs=10,\n",
    "    device='cuda',\n",
    "    method='sor',           # 'sor' or 'aom'\n",
    "    top_k=3                 # only for AOM\n",
    "):\n",
    "    from torch.utils.data import DataLoader\n",
    "    import os\n",
    "\n",
    "    # === Preprocess labeled dataset ===\n",
    "    train_dataset = ESC50Dataset(labeled_df, augment_type='weak')\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # === Optimizer and loss ===\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=3e-4)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    model.to(device)\n",
    "\n",
    "    # === Determine save path ===\n",
    "    save_dir = \"models/ESC-50/OSP\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    save_path = os.path.join(save_dir, f\"pretrained_{method}.pt\")\n",
    "\n",
    "    print(f\"Starting fine-tuning with ODC ({method.upper()})...\")\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss, total_cls, total_odc = 0, 0, 0\n",
    "        correct, total = 0, 0\n",
    "\n",
    "        for x, y in train_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "\n",
    "            # === Forward pass ===\n",
    "            feats = model(x, task='feature')\n",
    "            logits = model.classifier_head(feats)\n",
    "\n",
    "            # === Losses ===\n",
    "            loss_cls = criterion(logits, y)\n",
    "            loss_odc = odc_loss(model, feats, y, memory_bank, alpha=alpha, method=method, top_k=top_k)\n",
    "            loss = loss_cls + lambda_odc * loss_odc\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # === Logging ===\n",
    "            total_loss += loss.item()\n",
    "            total_cls += loss_cls.item()\n",
    "            total_odc += loss_odc.item()\n",
    "            correct += (logits.argmax(dim=1) == y).sum().item()\n",
    "            total += y.size(0)\n",
    "\n",
    "        acc = 100 * correct / total\n",
    "        print(f\"[Epoch {epoch+1}] Total Loss: {total_loss:.3f} | \"\n",
    "              f\"Cls Loss: {total_cls:.3f} | ODC Loss: {total_odc:.3f} | Acc: {acc:.2f}%\")\n",
    "\n",
    "    # === Save model ===\n",
    "    torch.save(model.state_dict(), save_path)\n",
    "    print(f\"Fine-tuned model saved to: {save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bc9d5551-bcc1-4ce6-a526-ae9e9e62f31d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-image\n",
      "  Downloading scikit_image-0.25.2-cp310-cp310-win_amd64.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy>=1.24 in c:\\users\\arnav\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from scikit-image) (1.26.4)\n",
      "Collecting scipy>=1.11.4 (from scikit-image)\n",
      "  Downloading scipy-1.15.2-cp310-cp310-win_amd64.whl.metadata (60 kB)\n",
      "Requirement already satisfied: networkx>=3.0 in c:\\users\\arnav\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from scikit-image) (3.3)\n",
      "Collecting pillow>=10.1 (from scikit-image)\n",
      "  Downloading pillow-11.2.1-cp310-cp310-win_amd64.whl.metadata (9.1 kB)\n",
      "Collecting imageio!=2.35.0,>=2.33 (from scikit-image)\n",
      "  Downloading imageio-2.37.0-py3-none-any.whl.metadata (5.2 kB)\n",
      "Collecting tifffile>=2022.8.12 (from scikit-image)\n",
      "  Downloading tifffile-2025.3.30-py3-none-any.whl.metadata (32 kB)\n",
      "Requirement already satisfied: packaging>=21 in c:\\users\\arnav\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from scikit-image) (23.0)\n",
      "Requirement already satisfied: lazy-loader>=0.4 in c:\\users\\arnav\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from scikit-image) (0.4)\n",
      "Downloading scikit_image-0.25.2-cp310-cp310-win_amd64.whl (12.8 MB)\n",
      "   ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "   ------------------ --------------------- 5.8/12.8 MB 25.1 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 11.5/12.8 MB 27.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 12.8/12.8 MB 25.9 MB/s eta 0:00:00\n",
      "Downloading imageio-2.37.0-py3-none-any.whl (315 kB)\n",
      "Downloading pillow-11.2.1-cp310-cp310-win_amd64.whl (2.7 MB)\n",
      "   ---------------------------------------- 0.0/2.7 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.7/2.7 MB 26.0 MB/s eta 0:00:00\n",
      "Downloading scipy-1.15.2-cp310-cp310-win_amd64.whl (41.2 MB)\n",
      "   ---------------------------------------- 0.0/41.2 MB ? eta -:--:--\n",
      "   ------- -------------------------------- 7.6/41.2 MB 36.2 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 14.7/41.2 MB 34.2 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 21.5/41.2 MB 34.0 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 27.5/41.2 MB 32.3 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 34.1/41.2 MB 32.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  40.6/41.2 MB 32.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  41.2/41.2 MB 32.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 41.2/41.2 MB 25.7 MB/s eta 0:00:00\n",
      "Downloading tifffile-2025.3.30-py3-none-any.whl (226 kB)\n",
      "Installing collected packages: tifffile, scipy, pillow, imageio, scikit-image\n",
      "  Attempting uninstall: scipy\n",
      "    Found existing installation: scipy 1.10.1\n",
      "    Uninstalling scipy-1.10.1:\n",
      "      Successfully uninstalled scipy-1.10.1\n",
      "  Attempting uninstall: pillow\n",
      "    Found existing installation: Pillow 9.4.0\n",
      "    Uninstalling Pillow-9.4.0:\n",
      "      Successfully uninstalled Pillow-9.4.0\n",
      "Successfully installed imageio-2.37.0 pillow-11.2.1 scikit-image-0.25.2 scipy-1.15.2 tifffile-2025.3.30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\Arnav\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\~cipy.libs'.\n",
      "  You can safely remove it manually.\n",
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\Arnav\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\~cipy'.\n",
      "  You can safely remove it manually.\n",
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\Arnav\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\~il'.\n",
      "  You can safely remove it manually.\n",
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.0.1\n",
      "[notice] To update, run: C:\\Users\\Arnav\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "33e81783-4a9c-4971-9bc1-ed5f0bb863bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Arnav\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Arnav\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting fine-tuning with ODC (AOM)...\n",
      "[Epoch 1] Total Loss: 6.531 | Cls Loss: 2.113 | ODC Loss: 8.837 | Acc: 95.94%\n",
      "[Epoch 2] Total Loss: 5.521 | Cls Loss: 1.646 | ODC Loss: 7.751 | Acc: 96.25%\n",
      "[Epoch 3] Total Loss: 4.615 | Cls Loss: 0.911 | ODC Loss: 7.409 | Acc: 97.50%\n",
      "[Epoch 4] Total Loss: 4.044 | Cls Loss: 0.507 | ODC Loss: 7.075 | Acc: 99.38%\n",
      "[Epoch 5] Total Loss: 4.081 | Cls Loss: 0.682 | ODC Loss: 6.798 | Acc: 99.06%\n",
      "[Epoch 6] Total Loss: 3.638 | Cls Loss: 0.454 | ODC Loss: 6.369 | Acc: 99.38%\n",
      "[Epoch 7] Total Loss: 3.726 | Cls Loss: 0.706 | ODC Loss: 6.041 | Acc: 98.44%\n",
      "[Epoch 8] Total Loss: 3.957 | Cls Loss: 1.000 | ODC Loss: 5.916 | Acc: 98.75%\n",
      "[Epoch 9] Total Loss: 3.380 | Cls Loss: 0.418 | ODC Loss: 5.923 | Acc: 99.06%\n",
      "[Epoch 10] Total Loss: 3.186 | Cls Loss: 0.323 | ODC Loss: 5.726 | Acc: 99.69%\n",
      "Fine-tuned model saved to: models/ESC-50/OSP\\pretrained_aom.pt\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = OSPModel(num_classes=len(label_map))\n",
    "checkpoint = torch.load(\"models/ESC-50/OSP/osp_pretrained.pt\", map_location='cpu')\n",
    "model.load_state_dict(checkpoint['model_state'])\n",
    "model.to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "labeled_df = pd.read_csv(\"osp-split/0.2/labeled0.3.csv\")\n",
    "labeled_df['target'] = labeled_df['target'].map(label_map)\n",
    "labeled_dataset = ESC50Dataset(labeled_df, augment_type='weak')\n",
    "labeled_loader = DataLoader(labeled_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "ood_features = features[ood_mask] \n",
    "\n",
    "centroids = compute_class_centroids(model, labeled_loader, device)\n",
    "\n",
    "memory_bank = build_ood_memory_bank(centroids, ood_features)\n",
    "\n",
    "train_with_odc(\n",
    "    model=model,  # pretrained and loaded\n",
    "    labeled_df=labeled_df,\n",
    "    memory_bank=memory_bank,\n",
    "    method='aom',      \n",
    "    top_k=3,\n",
    "    alpha=1.0,\n",
    "    lambda_odc=0.5,\n",
    "    num_epochs=10,\n",
    "    batch_size=32,\n",
    "    device='cuda' if torch.cuda.is_available() else 'cpu'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "673955e0-6255-43f1-9309-f331d55e95d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Arnav\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Arnav\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting fine-tuning with ODC (SOR)...\n",
      "[Epoch 1] Total Loss: 1.877 | Cls Loss: 1.249 | ODC Loss: 1.257 | Acc: 98.12%\n",
      "[Epoch 2] Total Loss: 0.820 | Cls Loss: 0.485 | ODC Loss: 0.672 | Acc: 99.38%\n",
      "[Epoch 3] Total Loss: 0.914 | Cls Loss: 0.707 | ODC Loss: 0.414 | Acc: 98.44%\n",
      "[Epoch 4] Total Loss: 0.493 | Cls Loss: 0.348 | ODC Loss: 0.291 | Acc: 99.38%\n",
      "[Epoch 5] Total Loss: 0.245 | Cls Loss: 0.144 | ODC Loss: 0.202 | Acc: 100.00%\n",
      "[Epoch 6] Total Loss: 0.381 | Cls Loss: 0.295 | ODC Loss: 0.171 | Acc: 99.69%\n",
      "[Epoch 7] Total Loss: 0.314 | Cls Loss: 0.254 | ODC Loss: 0.120 | Acc: 99.69%\n",
      "[Epoch 8] Total Loss: 0.206 | Cls Loss: 0.133 | ODC Loss: 0.145 | Acc: 99.69%\n",
      "[Epoch 9] Total Loss: 0.155 | Cls Loss: 0.078 | ODC Loss: 0.155 | Acc: 100.00%\n",
      "[Epoch 10] Total Loss: 0.101 | Cls Loss: 0.052 | ODC Loss: 0.098 | Acc: 100.00%\n",
      "Fine-tuned model saved to: models/ESC-50/OSP\\pretrained_sor.pt\n"
     ]
    }
   ],
   "source": [
    "# Reload the pretrained model from scratch\n",
    "model = OSPModel(num_classes=len(label_map))\n",
    "checkpoint = torch.load(\"models/ESC-50/OSP/osp_pretrained.pt\", map_location='cpu')\n",
    "model.load_state_dict(checkpoint['model_state'])\n",
    "model.to(device)\n",
    "\n",
    "# call train_with_odc with method='sor'\n",
    "train_with_odc(\n",
    "    model=model,\n",
    "    labeled_df=labeled_df,\n",
    "    memory_bank=memory_bank,\n",
    "    method='sor',\n",
    "    alpha=1.0,\n",
    "    lambda_odc=0.5,\n",
    "    num_epochs=10,\n",
    "    batch_size=32,\n",
    "    device=device\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8402ade6-4bfe-4900-aa7c-4060e072cff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score, average_precision_score\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "def evaluate_model_foldwise(model_path, meta_df, label_map, id_class_ids, batch_size=32, device='cuda'):\n",
    "    print(f\"\\nEvaluating model: {model_path}\")\n",
    "    num_classes = len(label_map)\n",
    "\n",
    "    all_fold_metrics = []\n",
    "\n",
    "    for fold in range(1, 6):  # ESC-50 has folds 1 to 5\n",
    "        print(f\"\\n=== Fold {fold} ===\")\n",
    "        model = OSPModel(num_classes=num_classes)\n",
    "        checkpoint = torch.load(model_path, map_location='cpu')\n",
    "        model.load_state_dict(checkpoint)\n",
    "        model.to(device)\n",
    "        model.eval()\n",
    "\n",
    "        # Prepare validation set (only ID classes)\n",
    "        val_df = meta_df[(meta_df['fold'] == fold) & (meta_df['target'].isin(id_class_ids))].copy()\n",
    "        val_df['target'] = val_df['target'].map(label_map)\n",
    "        val_dataset = ESC50Dataset(val_df, augment_type='none')\n",
    "        val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "        y_true, y_pred, y_probs = [], [], []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for x, y in tqdm(val_loader):\n",
    "                x, y = x.to(device), y.to(device)\n",
    "                logits = model(x, task='class')\n",
    "                preds = torch.argmax(logits, dim=1)\n",
    "                probs = torch.softmax(logits, dim=1)\n",
    "\n",
    "                y_true.extend(y.cpu().numpy())\n",
    "                y_pred.extend(preds.cpu().numpy())\n",
    "                y_probs.extend(probs.cpu().numpy())\n",
    "\n",
    "        # Metrics\n",
    "        acc = accuracy_score(y_true, y_pred)\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='macro')\n",
    "\n",
    "        try:\n",
    "            auc_roc = roc_auc_score(\n",
    "                y_true, y_probs, multi_class='ovr', average='macro'\n",
    "            )\n",
    "        except:\n",
    "            auc_roc = None\n",
    "\n",
    "        try:\n",
    "            auc_prc = average_precision_score(\n",
    "                y_true, y_probs, average='macro'\n",
    "            )\n",
    "        except:\n",
    "            auc_prc = None\n",
    "\n",
    "        print(f\"Accuracy: {acc:.4f}\")\n",
    "        print(f\"Precision: {precision:.4f}\")\n",
    "        print(f\"Recall: {recall:.4f}\")\n",
    "        print(f\"F1 Score: {f1:.4f}\")\n",
    "        print(f\"AUC-PRC: {auc_prc if auc_prc else 'N/A'}\")\n",
    "        print(f\"AUC-ROC: {auc_roc if auc_roc else 'N/A'}\")\n",
    "\n",
    "        all_fold_metrics.append({\n",
    "            'Fold': fold,\n",
    "            'Accuracy': acc,\n",
    "            'Precision': precision,\n",
    "            'Recall': recall,\n",
    "            'F1': f1,\n",
    "            'AUC-PRC': auc_prc,\n",
    "            'AUC-ROC': auc_roc\n",
    "        })\n",
    "\n",
    "    # Summary\n",
    "    print(\"\\n=== Summary (Mean across 5 folds) ===\")\n",
    "    df = pd.DataFrame(all_fold_metrics)\n",
    "    print(df.mean(numeric_only=True))\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "caa67a08-8506-4fae-a91f-85ad0013b20b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating model: models/ESC-50/OSP/pretrained_aom.pt\n",
      "\n",
      "=== Fold 1 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Arnav\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Arnav\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [00:07<00:00,  1.26it/s]\n",
      "C:\\Users\\Arnav\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Arnav\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7344\n",
      "Precision: 0.7774\n",
      "Recall: 0.7344\n",
      "F1 Score: 0.7218\n",
      "AUC-PRC: 0.8390696293486579\n",
      "AUC-ROC: 0.9848557692307693\n",
      "\n",
      "=== Fold 2 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [00:08<00:00,  1.19it/s]\n",
      "C:\\Users\\Arnav\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Arnav\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7281\n",
      "Precision: 0.7880\n",
      "Recall: 0.7281\n",
      "F1 Score: 0.7279\n",
      "AUC-PRC: 0.8438539245124386\n",
      "AUC-ROC: 0.9751302083333334\n",
      "\n",
      "=== Fold 3 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [00:08<00:00,  1.19it/s]\n",
      "C:\\Users\\Arnav\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Arnav\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7844\n",
      "Precision: 0.8246\n",
      "Recall: 0.7844\n",
      "F1 Score: 0.7817\n",
      "AUC-PRC: 0.8914925922126921\n",
      "AUC-ROC: 0.9863581730769232\n",
      "\n",
      "=== Fold 4 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [00:08<00:00,  1.24it/s]\n",
      "C:\\Users\\Arnav\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Arnav\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7531\n",
      "Precision: 0.7909\n",
      "Recall: 0.7531\n",
      "F1 Score: 0.7462\n",
      "AUC-PRC: 0.8678602145231278\n",
      "AUC-ROC: 0.9865685096153847\n",
      "\n",
      "=== Fold 5 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [00:08<00:00,  1.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6781\n",
      "Precision: 0.7318\n",
      "Recall: 0.6781\n",
      "F1 Score: 0.6686\n",
      "AUC-PRC: 0.7771731502221461\n",
      "AUC-ROC: 0.9742988782051283\n",
      "\n",
      "=== Summary (Mean across 5 folds) ===\n",
      "Fold         3.000000\n",
      "Accuracy     0.735625\n",
      "Precision    0.782545\n",
      "Recall       0.735625\n",
      "F1           0.729240\n",
      "AUC-PRC      0.843890\n",
      "AUC-ROC      0.981442\n",
      "dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate AOM-finetuned model\n",
    "df_results_aom = evaluate_model_foldwise(\n",
    "    model_path=\"models/ESC-50/OSP/pretrained_aom.pt\",\n",
    "    meta_df=meta_df,\n",
    "    label_map=label_map,\n",
    "    id_class_ids=id_class_ids,\n",
    "    device='cuda' if torch.cuda.is_available() else 'cpu'\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "880bad48-812f-4abb-8753-be8115e53dde",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Arnav\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Arnav\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating model: models/ESC-50/OSP/pretrained_sor.pt\n",
      "\n",
      "=== Fold 1 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [00:07<00:00,  1.26it/s]\n",
      "C:\\Users\\Arnav\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Arnav\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7656\n",
      "Precision: 0.7996\n",
      "Recall: 0.7656\n",
      "F1 Score: 0.7629\n",
      "AUC-PRC: 0.8507135157109402\n",
      "AUC-ROC: 0.9820813301282051\n",
      "\n",
      "=== Fold 2 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [00:07<00:00,  1.28it/s]\n",
      "C:\\Users\\Arnav\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Arnav\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7656\n",
      "Precision: 0.7993\n",
      "Recall: 0.7656\n",
      "F1 Score: 0.7585\n",
      "AUC-PRC: 0.870835393614392\n",
      "AUC-ROC: 0.9828625801282052\n",
      "\n",
      "=== Fold 3 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [00:08<00:00,  1.17it/s]\n",
      "C:\\Users\\Arnav\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Arnav\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8250\n",
      "Precision: 0.8529\n",
      "Recall: 0.8250\n",
      "F1 Score: 0.8271\n",
      "AUC-PRC: 0.9092938541069735\n",
      "AUC-ROC: 0.9936197916666668\n",
      "\n",
      "=== Fold 4 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [00:07<00:00,  1.25it/s]\n",
      "C:\\Users\\Arnav\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Arnav\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7937\n",
      "Precision: 0.8168\n",
      "Recall: 0.7937\n",
      "F1 Score: 0.7897\n",
      "AUC-PRC: 0.8958705153195494\n",
      "AUC-ROC: 0.991736778846154\n",
      "\n",
      "=== Fold 5 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [00:07<00:00,  1.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7031\n",
      "Precision: 0.7610\n",
      "Recall: 0.7031\n",
      "F1 Score: 0.6994\n",
      "AUC-PRC: 0.8091459556359109\n",
      "AUC-ROC: 0.975360576923077\n",
      "\n",
      "=== Summary (Mean across 5 folds) ===\n",
      "Fold         3.000000\n",
      "Accuracy     0.770625\n",
      "Precision    0.805923\n",
      "Recall       0.770625\n",
      "F1           0.767534\n",
      "AUC-PRC      0.867172\n",
      "AUC-ROC      0.985132\n",
      "dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate SOR-finetuned model\n",
    "df_results_aom = evaluate_model_foldwise(\n",
    "    model_path=\"models/ESC-50/OSP/pretrained_sor.pt\",\n",
    "    meta_df=meta_df,\n",
    "    label_map=label_map,\n",
    "    id_class_ids=id_class_ids,\n",
    "    device='cuda' if torch.cuda.is_available() else 'cpu'\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e872023-b61a-4e48-b246-0693df94ed1d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (GPU)",
   "language": "python",
   "name": "gpu-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
