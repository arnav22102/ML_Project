{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","mount_file_id":"1-ojNBKmoBnMXjGDg1jCVpdY-zpKWBhZb","authorship_tag":"ABX9TyNjzTxsjbX/6W6UHsLvSW1T"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"whj2_S6e-9dK","executionInfo":{"status":"ok","timestamp":1744662478782,"user_tz":-330,"elapsed":13762,"user":{"displayName":"Huma Khan","userId":"17799756192322724229"}}},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torchvision\n","import torchvision.transforms as transforms\n","from torch.utils.data import DataLoader, SubsetRandomSampler\n","import numpy as np\n","import os\n","import torch.optim as optim\n","from torch.nn import functional as F\n","import matplotlib.pyplot as plt\n","from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n","from sklearn.model_selection import KFold\n","from copy import deepcopy\n","import logging\n","import json\n","import seaborn as sns\n","from pathlib import Path\n","import warnings\n","from typing import Tuple, Dict, Any, List\n","from torch.optim.lr_scheduler import OneCycleLR\n","import time\n","\n","# Additional import for dynamic output in Jupyter Notebook\n","from IPython.display import clear_output, display\n","\n","import logging\n","\n","# Reset handlers if needed (important in Jupyter or scripts with imported modules)\n","for handler in logging.root.handlers[:]:\n","    logging.root.removeHandler(handler)\n","\n","# Setup logging\n","logging.basicConfig(\n","    level=logging.INFO,\n","    format='%(asctime)s - %(levelname)s - %(message)s',\n","    handlers=[\n","        logging.FileHandler('training.log'),\n","        logging.StreamHandler()\n","    ]\n",")\n","\n","# Set device and random seeds\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","torch.manual_seed(42)\n","np.random.seed(42)\n","\n","\n","class EarlyStopping:\n","    def __init__(self, patience=7, min_delta=0, verbose=False):\n","        self.patience = patience\n","        self.min_delta = min_delta\n","        self.verbose = verbose\n","        self.counter = 0\n","        self.best_loss = None\n","        self.early_stop = False\n","        self.best_model = None\n","\n","    def __call__(self, val_loss, model):\n","        if self.best_loss is None:\n","            self.best_loss = val_loss\n","            self.best_model = deepcopy(model.state_dict())\n","        elif val_loss > self.best_loss - self.min_delta:\n","            self.counter += 1\n","            if self.verbose:\n","                logging.info(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n","            if self.counter >= self.patience:\n","                self.early_stop = True\n","        else:\n","            self.best_loss = val_loss\n","            self.best_model = deepcopy(model.state_dict())\n","            self.counter = 0\n","        return self.early_stop\n","\n","\n","class ModelCheckpoint:\n","    def __init__(self, save_dir: str, metric_name: str = 'val_loss', mode: str = 'min'):\n","        self.save_dir = Path(save_dir)\n","        self.save_dir.mkdir(parents=True, exist_ok=True)\n","        self.metric_name = metric_name\n","        self.mode = mode\n","        self.best_metric = float('inf') if mode == 'min' else float('-inf')\n","\n","    def __call__(self, model: nn.Module, current_metric: float, epoch: int) -> None:\n","        improved = (self.mode == 'min' and current_metric < self.best_metric) or \\\n","                   (self.mode == 'max' and current_metric > self.best_metric)\n","\n","        if improved:\n","            self.best_metric = current_metric\n","            checkpoint = {\n","                'epoch': epoch,\n","                'model_state_dict': model.state_dict(),\n","                f'best_{self.metric_name}': self.best_metric\n","            }\n","            torch.save(checkpoint, self.save_dir / f'best_model_{self.metric_name}.pth')\n","            logging.info(f'Saved new best model with {self.metric_name}: {self.best_metric:.4f}')\n","\n","\n","class ConfidenceBasedThresholding:\n","    def __init__(self, initial_threshold=0.95, momentum=0.999):\n","        self.threshold = initial_threshold\n","        self.momentum = momentum\n","\n","    def update(self, confidence_scores):\n","        with torch.no_grad():\n","            sorted_confidence = torch.sort(confidence_scores)[0]\n","            new_threshold = sorted_confidence[int(0.95 * len(sorted_confidence))]\n","            self.threshold = self.momentum * self.threshold + (1 - self.momentum) * new_threshold\n","\n","    def get_mask(self, confidence_scores):\n","        return confidence_scores >= self.threshold\n","\n","\n","class TrainingProgressTracker:\n","    def __init__(self, total_epochs, num_folds):\n","        self.start_time = None\n","        self.total_epochs = total_epochs\n","        self.num_folds = num_folds\n","        self.current_fold = 0\n","        self.current_epoch = 0\n","        self.epoch_times = []\n","\n","    def start_training(self):\n","        self.start_time = time.time()\n","        logging.info(f\"Training started at {time.strftime('%Y-%m-%d %H:%M:%S')}\")\n","\n","    def update_progress(self, fold, epoch):\n","        self.current_fold = fold + 1\n","        self.current_epoch = epoch + 1\n","\n","        # Calculate time metrics\n","        elapsed = time.time() - self.start_time\n","        avg_time_per_epoch = np.mean(self.epoch_times) if self.epoch_times else 0\n","        remaining_epochs = (self.num_folds - self.current_fold) * self.total_epochs + \\\n","                           (self.total_epochs - self.current_epoch)\n","        estimated_remaining = avg_time_per_epoch * remaining_epochs\n","\n","        # Clear the output below the cell and display the updated progress in real time\n","        clear_output(wait=True)\n","        print(f\"Progress: Fold {self.current_fold}/{self.num_folds} | Epoch {self.current_epoch}/{self.total_epochs}\")\n","        print(f\"Elapsed: {self.format_time(elapsed)} | Remaining: {self.format_time(estimated_remaining)}\")\n","\n","    def record_epoch_time(self, epoch_time):\n","        self.epoch_times.append(epoch_time)\n","\n","    @staticmethod\n","    def format_time(seconds):\n","        if seconds < 0:\n","            return \"--:--:--\"\n","        hours, rem = divmod(seconds, 3600)\n","        minutes, seconds = divmod(rem, 60)\n","        return f\"{int(hours):02}:{int(minutes):02}:{int(seconds):02}\"\n","\n","\n","class TrainingMistakeDetector:\n","    def __init__(self, num_classes):\n","        self.num_classes = num_classes\n","        self.previous_train_loss = None\n","        self.previous_val_loss = None\n","\n","    def check_common_issues(self, model, train_metrics, val_metrics, epoch):\n","        issues = []\n","\n","        # Ensure train_metrics is a valid dictionary before accessing it\n","        if not train_metrics or 'loss' not in train_metrics:\n","            issues.append(\"Training metrics not computed for this epoch!\")\n","            return issues\n","\n","        # Check for NaN values\n","        if torch.isnan(torch.tensor(train_metrics['loss'])):\n","            issues.append(\"NaN detected in training loss!\")\n","\n","        # Check for exploding gradients\n","        if self.check_exploding_gradients(model):\n","            issues.append(\"Potential exploding gradients detected!\")\n","\n","        # Check for validation loss divergence if metrics are available\n","        if epoch > 1 and val_metrics:\n","            if val_metrics['val_loss'] > 2 * train_metrics['loss']:\n","                issues.append(\"Validation loss significantly higher than training loss (possible overfitting)\")\n","\n","            if self.previous_train_loss is not None and train_metrics['loss'] > self.previous_train_loss * 1.5:\n","                issues.append(\"Training loss increased significantly from previous epoch\")\n","\n","        # Update previous values if available\n","        self.previous_train_loss = train_metrics.get('loss', None)\n","        self.previous_val_loss = val_metrics.get('val_loss', None) if val_metrics else None\n","\n","        return issues\n","\n","    @staticmethod\n","    def check_exploding_gradients(model, threshold=1e6):\n","        for name, param in model.named_parameters():\n","            if param.grad is not None and torch.any(torch.abs(param.grad) > threshold):\n","                return True\n","        return False\n","\n"]},{"cell_type":"code","source":["class TransformFix(object):\n","    def __init__(self):\n","        self.weak = transforms.Compose([\n","            transforms.RandomHorizontalFlip(),\n","            transforms.RandomCrop(96, padding=4),\n","            transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n","            transforms.RandomRotation(10),\n","            transforms.ToTensor(),\n","            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n","        ])\n","\n","        self.strong = transforms.Compose([\n","            transforms.RandomHorizontalFlip(),\n","            transforms.RandomCrop(96, padding=4),\n","            transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4),\n","            transforms.RandomRotation(20),\n","            transforms.RandAugment(num_ops=3, magnitude=15),\n","            transforms.ToTensor(),\n","            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n","        ])\n","\n","    def __call__(self, x):\n","        return self.weak(x), self.strong(x)\n","\n","\n","def get_stl10_dataloaders(batch_size=64):\n","    transform = TransformFix()\n","    data_dir = \"/content/drive/MyDrive\"\n","    os.makedirs(data_dir, exist_ok=True)\n","\n","    try:\n","        labeled_dataset = torchvision.datasets.STL10(\n","            root=data_dir, split='train', download=True, transform=transform\n","        )\n","        unlabeled_dataset = torchvision.datasets.STL10(\n","            root=data_dir, split='unlabeled', download=True, transform=transform\n","        )\n","        test_dataset = torchvision.datasets.STL10(\n","            root=data_dir, split='test', download=True,\n","            transform=transforms.Compose([\n","                transforms.ToTensor(),\n","                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n","            ])\n","        )\n","    except Exception as e:\n","        logging.error(f\"Error loading datasets: {str(e)}\")\n","        raise\n","\n","    # Create train-validation split\n","    train_size = int(0.8 * len(labeled_dataset))\n","    val_size = len(labeled_dataset) - train_size\n","    train_dataset, val_dataset = torch.utils.data.random_split(\n","        labeled_dataset, [train_size, val_size]\n","    )\n","\n","    # Create data loaders with error handling\n","    try:\n","        train_loader = DataLoader(\n","            train_dataset, batch_size=batch_size, shuffle=True, num_workers=4\n","        )\n","\n","        val_dataset.dataset.transform = transforms.Compose([\n","            transforms.ToTensor(),\n","            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n","        ])\n","\n","        val_loader = DataLoader(\n","            val_dataset, batch_size=batch_size, shuffle=False, num_workers=4\n","        )\n","\n","        unlabeled_loader = DataLoader(\n","            unlabeled_dataset, batch_size=batch_size, shuffle=True, num_workers=4\n","        )\n","\n","        test_loader = DataLoader(\n","            test_dataset, batch_size=batch_size, shuffle=False, num_workers=4\n","        )\n","    except Exception as e:\n","        logging.error(f\"Error creating dataloaders: {str(e)}\")\n","        raise\n","\n","    return train_loader, val_loader, unlabeled_loader, test_loader\n"],"metadata":{"id":"tUMQZKJY_Iip","executionInfo":{"status":"ok","timestamp":1744662478811,"user_tz":-330,"elapsed":18,"user":{"displayName":"Huma Khan","userId":"17799756192322724229"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["class ResNet18(nn.Module):\n","    def __init__(self, num_classes=10, dropout_rate=0.3):\n","        super().__init__()\n","        self.model = torchvision.models.resnet18(pretrained=True)\n","        self.model.fc = nn.Sequential(\n","            nn.Dropout(dropout_rate),\n","            nn.Linear(512, 256),\n","            nn.ReLU(),\n","            nn.Dropout(dropout_rate),\n","            nn.Linear(256, num_classes)\n","        )\n","\n","    def forward(self, x):\n","        return self.model(x)\n","\n","\n","class FlatMatchLoss(nn.Module):\n","    def __init__(self, model, rho=1e-2, threshold=0.95, num_classes=10, temperature=1.0, use_sam=True):\n","        super().__init__()\n","        self.model = model\n","        self.rho = rho\n","        self.threshold = threshold\n","        self.num_classes = num_classes\n","        self.temperature = temperature\n","        self.use_sam = use_sam\n","        self.cross_entropy = nn.CrossEntropyLoss(reduction='none')\n","        self.kl_div = nn.KLDivLoss(reduction='batchmean')  # KL divergence for cross-sharpness\n","\n","    def forward(self, logits_w, logits_s, inputs_u, model, device):\n","        # Standard pseudo-labeling\n","        with torch.no_grad():\n","            probs_w = torch.softmax(logits_w / self.temperature, dim=1)\n","            max_probs, pseudo_labels = torch.max(probs_w, dim=1)\n","            mask = max_probs.ge(self.threshold)\n","\n","        # Compute pseudo-label loss\n","        loss_pseudo = self.cross_entropy(logits_s, pseudo_labels)\n","        loss_pseudo = (loss_pseudo * mask.float()).mean()\n","\n","        # Flat loss (KL from uniform)\n","        flat_targets = torch.ones_like(logits_w) / self.num_classes\n","        probs_s = torch.softmax(logits_s / self.temperature, dim=1)\n","        flat_loss = -torch.mean(torch.sum(flat_targets * torch.log(probs_s + 1e-6), dim=1))\n","\n","        # Total = pseudo-label + flat loss\n","        total_loss = loss_pseudo + 0.1 * flat_loss\n","\n","        # Cross-sharpness with SAM-style θ̃\n","        if self.use_sam:\n","            # First backward pass: get gradient\n","            inputs_u = inputs_u.detach()\n","            inputs_u.requires_grad = True\n","            logits_u = model(inputs_u)\n","            probs_u = torch.softmax(logits_u, dim=1)\n","            uniform = torch.full_like(probs_u, 1.0 / self.num_classes)\n","            kl_flat = torch.sum(probs_u * torch.log(probs_u / uniform + 1e-6), dim=1).mean()\n","\n","            # Backward to get grad\n","            grad = torch.autograd.grad(kl_flat, model.parameters(), create_graph=True)\n","            grad_norm = torch.sqrt(sum([g.norm() ** 2 for g in grad])) + 1e-12\n","\n","            # Perturb parameters: θ̃ = θ + ρ * g / ||g||\n","            eps = []\n","            for p, g in zip(model.parameters(), grad):\n","                e = self.rho * g / grad_norm\n","                p.data.add_(e)\n","                eps.append(e)\n","\n","            # Forward with perturbed θ̃\n","            logits_tilde = model(inputs_u)\n","            probs_tilde = torch.log_softmax(logits_tilde, dim=1)\n","            probs_orig = torch.softmax(logits_u.detach(), dim=1)\n","            sharpness_loss = self.kl_div(probs_tilde, probs_orig)\n","\n","            # Restore original weights\n","            for p, e in zip(model.parameters(), eps):\n","                p.data.sub_(e)\n","\n","            total_loss += 0.5 * sharpness_loss\n","\n","        return total_loss, mask.float().mean()\n","\n","\n","class DistributionAlignment(nn.Module):\n","    def __init__(self, num_classes, momentum=0.999):\n","        super().__init__()\n","        self.num_classes = num_classes\n","        self.momentum = momentum\n","        self.register_buffer(\"p_model\", torch.ones(num_classes) / num_classes)\n","\n","    def forward(self, probs):\n","        with torch.no_grad():\n","            p_current = probs.mean(0)\n","            self.p_model = self.momentum * self.p_model + (1 - self.momentum) * p_current\n","\n","        qt = probs.mean(0)\n","        p_ratio = self.p_model / (qt + 1e-6)\n","        return probs * p_ratio.unsqueeze(0)\n","\n","\n","def find_learning_rate(model: nn.Module, train_loader: DataLoader,\n","                       criterion: nn.Module, device: torch.device,\n","                       start_lr: float = 1e-7, end_lr: float = 10,\n","                       num_iterations: int = 100) -> Tuple[list, list]:\n","    logging.info(\"Starting learning rate finder...\")\n","\n","    model.train()\n","    optimizer = optim.SGD(model.parameters(), lr=start_lr)\n","    scheduler = optim.lr_scheduler.ExponentialLR(\n","        optimizer,\n","        gamma=(end_lr/start_lr)**(1/num_iterations)\n","    )\n","\n","    learning_rates = []\n","    losses = []\n","\n","    try:\n","        for batch_idx, (inputs, targets) in enumerate(train_loader):\n","            if batch_idx >= num_iterations:\n","                break\n","\n","            inputs, targets = inputs.to(device), targets.to(device)\n","\n","            optimizer.zero_grad()\n","            outputs = model(inputs)\n","            loss = criterion(outputs, targets)\n","\n","            loss.backward()\n","            optimizer.step()\n","\n","            learning_rates.append(optimizer.param_groups[0]['lr'])\n","            losses.append(loss.item())\n","\n","            scheduler.step()\n","\n","            if loss.item() > 4 * min(losses) or not np.isfinite(loss.item()):\n","                break\n","\n","    except Exception as e:\n","        logging.error(f\"Error during learning rate finding: {str(e)}\")\n","        raise\n","\n","    return learning_rates, losses\n","\n","\n","def validate(model: nn.Module, val_loader: DataLoader,\n","             criterion: nn.Module, device: torch.device) -> Dict[str, float]:\n","    model.eval()\n","    val_loss = 0\n","    correct = 0\n","    total = 0\n","    all_preds = []\n","    all_targets = []\n","\n","    with torch.no_grad():\n","        for inputs, targets in val_loader:\n","            inputs, targets = inputs.to(device), targets.to(device)\n","            outputs = model(inputs)\n","            loss = criterion(outputs, targets)\n","\n","            val_loss += loss.item()\n","            _, predicted = outputs.max(1)\n","            total += targets.size(0)\n","            correct += predicted.eq(targets).sum().item()\n","\n","            all_preds.extend(predicted.cpu().numpy())\n","            all_targets.extend(targets.cpu().numpy())\n","\n","    metrics = {\n","        'val_loss': val_loss / len(val_loader),\n","        'val_acc': correct / total,\n","        'val_precision': precision_score(all_targets, all_preds, average='macro'),\n","        'val_recall': recall_score(all_targets, all_preds, average='macro'),\n","        'val_f1': f1_score(all_targets, all_preds, average='macro')\n","    }\n","\n","    return metrics\n","\n","\n","\n","\n","\n","def train_epoch(model, model_ema, labeled_loader, unlabeled_loader, optimizer, dist_align,\n","                flatmatch_loss, confidence_thresholder, device, temperature=1.0, lambda_u=1.0):\n","    model.train()\n","    dist_align.train()\n","    total_loss = 0\n","    correct = 0\n","    total = 0\n","    unlabeled_iter = iter(unlabeled_loader)\n","\n","    for batch_idx, (x_l, y_l) in enumerate(labeled_loader):\n","        try:\n","            (u_w, u_s), _ = next(unlabeled_iter)\n","        except StopIteration:\n","            unlabeled_iter = iter(unlabeled_loader)\n","            (u_w, u_s), _ = next(unlabeled_iter)\n","\n","        x_l, y_l = x_l.to(device), y_l.to(device)\n","        u_w, u_s = u_w.to(device), u_s.to(device)\n","\n","        logits_l = model(x_l)\n","        logits_u_w = model(u_w)\n","        logits_u_s = model(u_s)\n","\n","        loss_sup = F.cross_entropy(logits_l, y_l)\n","\n","        _, predicted = torch.max(logits_l.data, 1)\n","        total += y_l.size(0)\n","        correct += (predicted == y_l).sum().item()\n","\n","        with torch.no_grad():\n","            probs_u_w = torch.softmax(logits_u_w / temperature, dim=1)\n","            max_probs, _ = torch.max(probs_u_w, dim=1)\n","            confidence_thresholder.update(max_probs)\n","            conf_mask = confidence_thresholder.get_mask(max_probs)\n","            probs_u_w = dist_align(probs_u_w)\n","\n","        loss_unsup, mask_mean = flatmatch_loss(\n","            logits_u_w, logits_u_s, u_w, model, device\n","        )\n","        loss_unsup = loss_unsup * conf_mask.float().mean()\n","\n","        loss = loss_sup + lambda_u * loss_unsup\n","\n","        optimizer.zero_grad()\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n","        optimizer.step()\n","\n","        update_ema(model, model_ema)\n","\n","        total_loss += loss.item()\n","\n","    return {\n","        'loss': total_loss / (batch_idx + 1),\n","        'acc': correct / total,\n","        'confidence_threshold': confidence_thresholder.threshold\n","    }\n","\n"],"metadata":{"id":"mCy2mTHP_LHB","executionInfo":{"status":"ok","timestamp":1744662486311,"user_tz":-330,"elapsed":68,"user":{"displayName":"Huma Khan","userId":"17799756192322724229"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["def update_ema(model, model_ema, alpha=0.999):\n","    for ema_param, param in zip(model_ema.parameters(), model.parameters()):\n","        ema_param.data.mul_(alpha).add_(param.data, alpha=1 - alpha)\n","\n","def train_with_kfold_improved(model, labeled_dataset, unlabeled_loader, num_folds=5, num_epochs=200, device=device):\n","    # Initialize progress tracker and mistake detector only once\n","    progress_tracker = TrainingProgressTracker(num_epochs, num_folds)\n","    progress_tracker.start_training()\n","    mistake_detector = TrainingMistakeDetector(num_classes=10)\n","\n","    kfold = KFold(n_splits=num_folds, shuffle=True, random_state=42)\n","    fold_results = []\n","    best_val_acc = 0\n","    best_model = None\n","\n","    # Create checkpoint handler\n","    checkpoint_handler = ModelCheckpoint(save_dir='checkpoints', metric_name='val_acc', mode='max')\n","\n","    for fold, (train_ids, val_ids) in enumerate(kfold.split(labeled_dataset)):\n","        logging.info(f'\\nStarting Fold {fold+1}/{num_folds}')\n","\n","        # Create data loaders for the current fold\n","        train_loader = DataLoader(\n","            labeled_dataset,\n","            batch_size=64,\n","            sampler=SubsetRandomSampler(train_ids)\n","        )\n","        val_loader = DataLoader(\n","            labeled_dataset,\n","            batch_size=64,\n","            sampler=SubsetRandomSampler(val_ids)\n","        )\n","\n","        # Initialize model and components for the current fold\n","        fold_model = ResNet18().to(device)\n","        model_ema = deepcopy(fold_model)\n","        for param in model_ema.parameters():\n","          param.requires_grad = False\n","        confidence_thresholder = ConfidenceBasedThresholding(initial_threshold=0.95)\n","        dist_align = DistributionAlignment(num_classes=10).to(device)\n","        flatmatch_loss = FlatMatchLoss(model=fold_model,threshold=0.95, num_classes=10, temperature=1.0).to(device)\n","\n","        # Find optimal learning rate\n","        lrs, losses = find_learning_rate(fold_model, train_loader, nn.CrossEntropyLoss(), device)\n","        optimal_lr = lrs[np.argmin(losses)]\n","        logging.info(f\"Optimal learning rate found: {optimal_lr:.6f}\")\n","\n","        # Initialize optimizer and scheduler\n","        optimizer = optim.SGD([\n","            {'params': fold_model.model.fc.parameters(), 'lr': optimal_lr},\n","            {'params': [p for n, p in fold_model.model.named_parameters() if not n.startswith('fc')],\n","             'lr': optimal_lr/10}\n","        ], momentum=0.9, weight_decay=1e-4)\n","\n","        scheduler = OneCycleLR(optimizer, max_lr=optimal_lr, epochs=num_epochs,\n","                                steps_per_epoch=len(train_loader))\n","        early_stopping = EarlyStopping(patience=10, verbose=True)\n","\n","        # Training loop for the current fold\n","        fold_train_losses = []\n","        fold_val_losses = []\n","        fold_train_accs = []\n","        fold_val_accs = []\n","\n","        for epoch in range(num_epochs):\n","            epoch_start_time = time.time()\n","            try:\n","                train_metrics = train_epoch(\n","                    fold_model, model_ema, train_loader, unlabeled_loader,\n","                    optimizer, dist_align, flatmatch_loss,\n","                    confidence_thresholder, device\n","                    )\n","\n","\n","                # Validation phase\n","                val_metrics = validate(fold_model, val_loader, nn.CrossEntropyLoss(), device)\n","                epoch_time = time.time() - epoch_start_time\n","                progress_tracker.record_epoch_time(epoch_time)\n","                progress_tracker.update_progress(fold, epoch)\n","\n","                issues = mistake_detector.check_common_issues(\n","                    fold_model, train_metrics, val_metrics, epoch\n","                )\n","\n","                if issues:\n","                    logging.warning(\"Potential issues detected:\")\n","                    for issue in issues:\n","                        logging.warning(f\"  - {issue}\")\n","\n","                # Store metrics\n","                fold_train_losses.append(train_metrics['loss'])\n","                fold_train_accs.append(train_metrics['acc'])\n","                fold_val_losses.append(val_metrics['val_loss'])\n","                fold_val_accs.append(val_metrics['val_acc'])\n","\n","                # Log metrics\n","                logging.info(f'Epoch {epoch+1}/{num_epochs}:')\n","                logging.info(f'Train Loss: {train_metrics[\"loss\"]:.4f}, Train Acc: {train_metrics[\"acc\"]:.4f}')\n","                logging.info(f'Confidence Threshold: {train_metrics[\"confidence_threshold\"]:.4f}')\n","                logging.info(f'Val Loss: {val_metrics[\"val_loss\"]:.4f}, Val Acc: {val_metrics[\"val_acc\"]:.4f}')\n","\n","                # Checkpoint saving\n","                checkpoint_handler(fold_model, val_metrics[\"val_acc\"], epoch)\n","\n","                # Early stopping check\n","                if early_stopping(val_metrics[\"val_loss\"], fold_model):\n","                    logging.info(\"Early stopping triggered!\")\n","                    fold_model.load_state_dict(early_stopping.best_model)\n","                    break\n","\n","                scheduler.step()\n","\n","            except Exception as e:\n","                logging.error(f\"Error during training: {str(e)}\")\n","                logging.error(f\"Error occurred at Fold {fold+1}, Epoch {epoch+1}\")\n","                logging.error(f\"Current learning rate: {optimizer.param_groups[0]['lr']}\")\n","                raise\n","\n","        # Store fold results\n","        fold_results.append({\n","            'model': fold_model,\n","            'val_metrics': val_metrics,\n","            'train_losses': fold_train_losses,\n","            'val_losses': fold_val_losses,\n","            'train_accs': fold_train_accs,\n","            'val_accs': fold_val_accs\n","        })\n","\n","        # Update best model\n","        if val_metrics['val_acc'] > best_val_acc:\n","            best_val_acc = val_metrics['val_acc']\n","            best_model = deepcopy(fold_model)\n","\n","    return best_model, fold_results\n","\n","\n","def plot_training_curves(fold_results, save_dir):\n","    plt.figure(figsize=(15, 10))\n","\n","    # Plot losses\n","    plt.subplot(2, 2, 1)\n","    for fold_idx, fold_data in enumerate(fold_results):\n","        plt.plot(fold_data['train_losses'], label=f'Fold {fold_idx+1} Train')\n","        plt.plot(fold_data['val_losses'], label=f'Fold {fold_idx+1} Val')\n","    plt.title('Training and Validation Loss')\n","    plt.xlabel('Epoch')\n","    plt.ylabel('Loss')\n","    plt.legend()\n","\n","    # Plot accuracies\n","    plt.subplot(2, 2, 2)\n","    for fold_idx, fold_data in enumerate(fold_results):\n","        plt.plot(fold_data['train_accs'], label=f'Fold {fold_idx+1} Train')\n","        plt.plot(fold_data['val_accs'], label=f'Fold {fold_idx+1} Val')\n","    plt.title('Training and Validation Accuracy')\n","    plt.xlabel('Epoch')\n","    plt.ylabel('Accuracy')\n","    plt.legend()\n","\n","    plt.tight_layout()\n","    plt.savefig(os.path.join(save_dir, 'training_curves.png'))\n","    plt.close()\n","\n","\n","def plot_confusion_matrix(model, test_loader, device, save_dir):\n","    model.eval()\n","    all_preds = []\n","    all_labels = []\n","\n","    with torch.no_grad():\n","        for images, labels in test_loader:\n","            images = images.to(device)\n","            outputs = model(images)\n","            _, predicted = torch.max(outputs, 1)\n","            all_preds.extend(predicted.cpu().numpy())\n","            all_labels.extend(labels.numpy())\n","\n","    cm = confusion_matrix(all_labels, all_preds)\n","    plt.figure(figsize=(10, 8))\n","    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n","    plt.title('Confusion Matrix')\n","    plt.xlabel('Predicted')\n","    plt.ylabel('True')\n","    plt.savefig(os.path.join(save_dir, 'confusion_matrix.png'))\n","    plt.close()\n","\n","\n","def main():\n","    # Set save directory\n","    save_dir = \"/content/drive/MyDrive/flatmatch-stl-10-results-trial\"\n","    os.makedirs(save_dir, exist_ok=True)\n","\n","    try:\n","        # Load data\n","        train_loader, val_loader, unlabeled_loader, test_loader = get_stl10_dataloaders()\n","        logging.info(\"Data loaded successfully\")\n","\n","        # Train model with k-fold cross-validation\n","        best_model, fold_results = train_with_kfold_improved(\n","            model=None,\n","            labeled_dataset=train_loader.dataset,\n","            unlabeled_loader=unlabeled_loader,\n","            num_folds=5,\n","            num_epochs=120,\n","            device=device\n","        )\n","\n","        # Evaluate on test set\n","        test_metrics = validate(best_model, test_loader, nn.CrossEntropyLoss(), device)\n","\n","        # Plot results\n","        plot_training_curves(fold_results, save_dir)\n","        plot_confusion_matrix(best_model, test_loader, device, save_dir)\n","\n","        # Save model and results\n","        torch.save({\n","            'model_state_dict': best_model.state_dict(),\n","            'test_metrics': test_metrics\n","        }, os.path.join(save_dir, 'best_model_flatmatch_stl-10.pth'))\n","\n","        # Log final results\n","        logging.info(\"\\nFinal Test Metrics:\")\n","        logging.info(f\"Accuracy: {test_metrics['val_acc']:.4f}\")\n","        logging.info(f\"Precision: {test_metrics['val_precision']:.4f}\")\n","        logging.info(f\"Recall: {test_metrics['val_recall']:.4f}\")\n","        logging.info(f\"F1 Score: {test_metrics['val_f1']:.4f}\")\n","\n","        # Log final results using print\n","        print(\"\\nFinal Test Metrics:\")\n","        print(f\"Accuracy: {test_metrics['val_acc']:.4f}\")\n","        print(f\"Precision: {test_metrics['val_precision']:.4f}\")\n","        print(f\"Recall: {test_metrics['val_recall']:.4f}\")\n","        print(f\"F1 Score: {test_metrics['val_f1']:.4f}\")\n","\n","\n","        return best_model, test_metrics\n","\n","    except Exception as e:\n","        logging.error(f\"Error in main pipeline: {str(e)}\")\n","        raise\n","\n","\n","if __name__ == \"__main__\":\n","    warnings.filterwarnings('ignore')\n","    model, metrics = main()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Va6EQosV_NuK","executionInfo":{"status":"ok","timestamp":1744673480651,"user_tz":-330,"elapsed":4946980,"user":{"displayName":"Huma Khan","userId":"17799756192322724229"}},"outputId":"df1b5156-69a8-4ec8-ee6e-1fc5039eb002"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stderr","text":["2025-04-14 23:31:39,302 - INFO - Epoch 21/120:\n","2025-04-14 23:31:39,303 - INFO - Train Loss: 2.3663, Train Acc: 0.1084\n","2025-04-14 23:31:39,305 - INFO - Confidence Threshold: 0.5028\n","2025-04-14 23:31:39,306 - INFO - Val Loss: 2.3119, Val Acc: 0.1050\n","2025-04-14 23:31:39,306 - INFO - EarlyStopping counter: 10 out of 10\n","2025-04-14 23:31:39,307 - INFO - Early stopping triggered!\n"]},{"output_type":"stream","name":"stdout","text":["Progress: Fold 5/5 | Epoch 21/120\n","Elapsed: 01:21:41 | Remaining: 00:28:05\n"]},{"output_type":"stream","name":"stderr","text":["2025-04-14 23:31:51,806 - INFO - \n","Final Test Metrics:\n","2025-04-14 23:31:51,807 - INFO - Accuracy: 0.5499\n","2025-04-14 23:31:51,809 - INFO - Precision: 0.5735\n","2025-04-14 23:31:51,809 - INFO - Recall: 0.5499\n","2025-04-14 23:31:51,810 - INFO - F1 Score: 0.5294\n"]},{"output_type":"stream","name":"stdout","text":["\n","Final Test Metrics:\n","Accuracy: 0.5499\n","Precision: 0.5735\n","Recall: 0.5499\n","F1 Score: 0.5294\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"1GaN2Fbi_amM"},"execution_count":null,"outputs":[]}]}