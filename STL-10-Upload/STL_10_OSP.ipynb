{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader, Subset, ConcatDataset\n",
        "import numpy as np\n",
        "from sklearn.model_selection import KFold\n",
        "from collections import deque\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "from copy import deepcopy\n",
        "from google.colab import drive  # For Google Drive integration\n",
        "\n",
        "\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'Using device: {device}')\n",
        "\n",
        "# Hyperparameters\n",
        "K_FOLDS = 5\n",
        "BATCH_SIZE = 64\n",
        "NUM_CLASSES = 10\n",
        "MEMORY_SIZE = 512  # Size of OOD memory bank per class\n",
        "ALPHA = 0.8       # Orthogonal decomposition ratio\n",
        "THRESHOLD = 0.8   # Confidence threshold\n",
        "PATIENCE = 5      # Early stopping patience\n",
        "SAVE_DIR = '/content/drive/MyDrive/OSP_Models'  # Directory to save models\n",
        "\n",
        "# Create save directory if it doesn't exist\n",
        "os.makedirs(SAVE_DIR, exist_ok=True)\n",
        "\n",
        "# Data Augmentation Transforms\n",
        "weak_transform = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomCrop(96, padding=4),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616))\n",
        "])\n",
        "\n",
        "strong_transform = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomCrop(96, padding=4),\n",
        "    transforms.RandomAffine(degrees=30, translate=(0.2, 0.2)),\n",
        "    transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616))\n",
        "])\n",
        "\n",
        "# Rotation Transformation\n",
        "class RotationTransform:\n",
        "    def __init__(self):\n",
        "        self.angles = [0, 90, 180, 270]\n",
        "\n",
        "    def __call__(self, x):\n",
        "        return [transforms.functional.rotate(x, angle) for angle in self.angles]\n",
        "\n",
        "# Model Architecture\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.resnet = torchvision.models.resnet18(pretrained=False)\n",
        "        self.resnet.fc = nn.Identity()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.resnet(x)\n",
        "\n",
        "class OSPModel(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super().__init__()\n",
        "        self.encoder = Encoder()\n",
        "        self.classifier = nn.Linear(512, num_classes)\n",
        "        self.rotation_head = nn.Linear(512, 4)\n",
        "        self.ood_detector = nn.Linear(512, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = self.encoder(x)\n",
        "        return {\n",
        "            'cls': self.classifier(features),\n",
        "            'rot': self.rotation_head(features),\n",
        "            'ood': torch.sigmoid(self.ood_detector(features))\n",
        "        }\n",
        "\n",
        "# OSP Components\n",
        "class AOM:\n",
        "    def __init__(self, num_classes, memory_size):\n",
        "        self.memory_banks = [deque(maxlen=memory_size) for _ in range(num_classes)]\n",
        "\n",
        "    def update_memory(self, features, pseudo_labels, ood_scores):\n",
        "        for feat, pl, score in zip(features, pseudo_labels, ood_scores):\n",
        "            if score < 0.2:  # OOD threshold\n",
        "                self.memory_banks[pl].append(feat.detach().cpu())\n",
        "\n",
        "    def get_ood_pair(self, class_id):\n",
        "        if len(self.memory_banks[class_id]) == 0:\n",
        "            return None\n",
        "        ood_pairs = np.vstack(self.memory_banks[class_id])\n",
        "        selected_index = np.random.choice(ood_pairs.shape[0])\n",
        "        return ood_pairs[selected_index]\n",
        "\n",
        "class SOR(nn.Module):\n",
        "    def __init__(self, alpha):\n",
        "        super().__init__()\n",
        "        self.alpha = alpha\n",
        "\n",
        "    def forward(self, z_id, z_ood):\n",
        "        # Compute cosine similarity\n",
        "        cos_sim = nn.functional.cosine_similarity(z_id, z_ood, dim=1,eps=1e-6)\n",
        "\n",
        "        # Projection magnitude\n",
        "        proj_mag = torch.norm(z_id, dim=1,keepdim=True) * cos_sim.unsqueeze(1)\n",
        "\n",
        "        # Direction vectors\n",
        "        z_ood_unit = z_ood / (torch.norm(z_ood, dim=1, keepdim=True) + 1e-6)\n",
        "        proj = proj_mag * z_ood_unit\n",
        "\n",
        "        # Orthogonal decomposition\n",
        "        z_pruned = z_id - self.alpha * proj\n",
        "        return z_pruned\n",
        "\n",
        "# Loss Functions\n",
        "class OSPLoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.ce = nn.CrossEntropyLoss()\n",
        "        self.kl = nn.KLDivLoss(reduction='batchmean')\n",
        "\n",
        "    def forward(self, outputs, labels, pruned_outputs, ood_scores):\n",
        "        # Classification loss\n",
        "        loss_cls = self.ce(outputs['cls'], labels)\n",
        "\n",
        "        # Orthogonality regularization\n",
        "        loss_ortho = self.kl(\n",
        "            nn.functional.log_softmax(pruned_outputs, dim=1),\n",
        "            nn.functional.softmax(outputs['cls'], dim=1)\n",
        "        )\n",
        "\n",
        "        # OOD detection loss\n",
        "        loss_ood = nn.BCELoss()(outputs['ood'], ood_scores)\n",
        "\n",
        "        return loss_cls + 0.5*loss_ortho + 0.1*loss_ood\n",
        "\n",
        "# Early Stopping\n",
        "class EarlyStopping:\n",
        "    def __init__(self, patience=5, min_delta=0):\n",
        "        self.patience = patience\n",
        "        self.min_delta = min_delta\n",
        "        self.counter = 0\n",
        "        self.best_loss = None\n",
        "        self.early_stop = False\n",
        "\n",
        "    def __call__(self, val_loss):\n",
        "        if self.best_loss is None:\n",
        "            self.best_loss = val_loss\n",
        "        elif val_loss > self.best_loss - self.min_delta:\n",
        "            self.counter += 1\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            self.best_loss = val_loss\n",
        "            self.counter = 0\n",
        "        return self.early_stop\n",
        "\n",
        "# Data Preparation\n",
        "def prepare_data():\n",
        "    # Labeled data\n",
        "    labeled_dataset = torchvision.datasets.STL10(\n",
        "        root='./data', split='train', download=True,\n",
        "        transform=transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616))\n",
        "        ])\n",
        "    )\n",
        "\n",
        "    # Unlabeled data (contains OOD samples)\n",
        "    unlabeled_dataset = torchvision.datasets.STL10(\n",
        "        root='./data', split='unlabeled', download=True,\n",
        "        transform=transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616))\n",
        "        ])\n",
        "    )\n",
        "\n",
        "    return labeled_dataset, unlabeled_dataset\n",
        "\n",
        "# Training Loop\n",
        "def train_osp():\n",
        "    labeled_dataset, unlabeled_dataset = prepare_data()\n",
        "    kfold = KFold(n_splits=K_FOLDS, shuffle=True)\n",
        "    fold_results = []\n",
        "\n",
        "    for fold, (train_idx, val_idx) in enumerate(kfold.split(labeled_dataset)):\n",
        "        print(f'\\nFold {fold+1}/{K_FOLDS}')\n",
        "        print('-'*20)\n",
        "\n",
        "        # Data Loaders\n",
        "        train_loader = DataLoader(\n",
        "            Subset(labeled_dataset, train_idx),\n",
        "            batch_size=BATCH_SIZE, shuffle=True\n",
        "        )\n",
        "        val_loader = DataLoader(\n",
        "            Subset(labeled_dataset, val_idx),\n",
        "            batch_size=BATCH_SIZE\n",
        "        )\n",
        "        unlabeled_loader = DataLoader(\n",
        "            unlabeled_dataset,\n",
        "            batch_size=BATCH_SIZE*5, shuffle=True\n",
        "        )\n",
        "\n",
        "        # Initialize components\n",
        "        model = OSPModel(NUM_CLASSES).to(device)\n",
        "        aom = AOM(NUM_CLASSES, MEMORY_SIZE)\n",
        "        sor = SOR(ALPHA).to(device)\n",
        "        criterion = OSPLoss()\n",
        "        optimizer = torch.optim.SGD(model.parameters(), lr=0.03, momentum=0.9, weight_decay=5e-4)\n",
        "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=100)\n",
        "\n",
        "        # Training history\n",
        "        history = {'train_loss': [], 'val_loss': [], 'train_acc': [], 'val_acc': []}\n",
        "\n",
        "        # Early Stopping\n",
        "        early_stopping = EarlyStopping(patience=PATIENCE)\n",
        "\n",
        "        # Best model tracking\n",
        "        best_val_acc = 0\n",
        "        best_model_state = None\n",
        "\n",
        "        # Pre-training Stage\n",
        "        print('Pre-training Stage:')\n",
        "        for epoch in range(2):\n",
        "            model.train()\n",
        "            total_loss = 0\n",
        "            correct = 0\n",
        "            total = 0\n",
        "\n",
        "            for batch_idx, ((inputs, labels), (unlabeled, _)) in enumerate(zip(train_loader, unlabeled_loader)):\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "                unlabeled = unlabeled.to(device)\n",
        "\n",
        "                # Rotation prediction\n",
        "                rotated = RotationTransform()(unlabeled)\n",
        "                rotation_labels = torch.LongTensor([i//4 for i in range(4)]).repeat(unlabeled.size(0))\n",
        "\n",
        "                # Forward pass\n",
        "                rot_features = []\n",
        "                for r in rotated:\n",
        "                  rot_features.append(model.encoder(r.to(device)))\n",
        "                rot_outputs = model.rotation_head(torch.cat(rot_features))\n",
        "                outputs = model(inputs)\n",
        "                # Loss calculation\n",
        "                loss_cls = criterion.ce(outputs['cls'], labels)\n",
        "                loss_rot = criterion.ce(rot_outputs, rotation_labels.to(device))\n",
        "                loss = loss_cls + 0.5*loss_rot\n",
        "\n",
        "                # Backprop\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                # Metrics\n",
        "                total_loss += loss.item()\n",
        "                _, predicted = outputs['cls'].max(1)\n",
        "                total += labels.size(0)\n",
        "                correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "            # Validation\n",
        "            val_loss, val_acc = validate(model, val_loader)\n",
        "            if total == 0:\n",
        "              print(\"Warning: Total samples processed is 0. Train accuracy will be set to 0.\")\n",
        "              train_acc = 0\n",
        "            else:\n",
        "              train_acc = 100. * correct / total  # Calculate train accuracy\n",
        "            history['train_loss'].append(total_loss/len(train_loader))\n",
        "            history['train_acc'].append(train_acc)\n",
        "            history['val_loss'].append(val_loss)\n",
        "            history['val_acc'].append(val_acc)\n",
        "\n",
        "            print(f'Epoch {epoch+1}/50 | Loss: {history[\"train_loss\"][-1]:.3f} | '\n",
        "                  f'Acc: {history[\"train_acc\"][-1]:.2f}% | Val Acc: {val_acc:.2f}%')\n",
        "\n",
        "        # Fine-tuning Stage with OSP\n",
        "        print('\\nFine-tuning Stage:')\n",
        "        optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=100)\n",
        "\n",
        "        for epoch in range(2):\n",
        "            model.train()\n",
        "            total_loss = 0\n",
        "            correct = 0\n",
        "            total = 0\n",
        "\n",
        "            for batch_idx, ((inputs, labels), (unlabeled, _)) in enumerate(zip(train_loader, unlabeled_loader)):\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "                unlabeled = unlabeled.to(device)\n",
        "\n",
        "                # Forward pass\n",
        "                outputs = model(inputs)\n",
        "                with torch.no_grad():\n",
        "                    ood_outputs = model(unlabeled)\n",
        "\n",
        "                # OOD detection\n",
        "                ood_scores = (ood_outputs['ood'] < 0.5).float()\n",
        "                pseudo_labels = ood_outputs['cls'].argmax(1)\n",
        "\n",
        "                # Update AOM memory banks\n",
        "                aom.update_memory(\n",
        "                    model.encoder(unlabeled),\n",
        "                    pseudo_labels.cpu(),\n",
        "                    ood_scores.cpu()\n",
        "                )\n",
        "\n",
        "                # Get ID-OOD pairs\n",
        "                batch_ood = []\n",
        "                for c in labels.unique():\n",
        "                    class_mask = (labels == c)\n",
        "                    class_features = model.encoder(inputs[class_mask])\n",
        "                    for feat in class_features:\n",
        "                        ood_feat = aom.get_ood_pair(c.item())\n",
        "                        if ood_feat is not None:\n",
        "                            batch_ood.append(torch.tensor(ood_feat).to(device))\n",
        "\n",
        "                if len(batch_ood) == 0:\n",
        "                    continue\n",
        "\n",
        "                # Apply SOR\n",
        "                if len(batch_ood) > 0:\n",
        "                  if len(batch_ood) < inputs.shape[0]:\n",
        "                    padding_size =  inputs.shape[0] - len(batch_ood)\n",
        "                    batch_ood.extend([batch_ood[-1]] * padding_size)  #\n",
        "                  batch_ood_tensor = torch.stack(batch_ood).to(device)\n",
        "                  batch_ood_tensor = batch_ood_tensor.view(inputs.shape[0], -1)\n",
        "                  pruned_features = sor(\n",
        "                      model.encoder(inputs),\n",
        "                      batch_ood_tensor)\n",
        "                  pruned_outputs = model.classifier(pruned_features)\n",
        "                else:\n",
        "                  pruned_outputs = outputs['cls']\n",
        "\n",
        "                # Loss calculation\n",
        "                ood_scores_resized = ood_scores[:inputs.size(0)].to(device)\n",
        "                loss = criterion(outputs, labels, pruned_outputs, ood_scores_resized)\n",
        "\n",
        "                # Backprop\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                # Metrics\n",
        "                total_loss += loss.item()\n",
        "                _, predicted = outputs['cls'].max(1)\n",
        "                total += labels.size(0)\n",
        "                correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "            # Validation\n",
        "            val_loss, val_acc = validate(model, val_loader)\n",
        "            history['train_loss'].append(total_loss/len(train_loader))\n",
        "            history['train_acc'].append(100.*correct/total)\n",
        "            history['val_loss'].append(val_loss)\n",
        "            history['val_acc'].append(val_acc)\n",
        "\n",
        "            print(f'Epoch {epoch+1}/100 | Loss: {history[\"train_loss\"][-1]:.3f} | '\n",
        "                  f'Acc: {history[\"train_acc\"][-1]:.2f}% | Val Acc: {val_acc:.2f}%')\n",
        "            scheduler.step()\n",
        "\n",
        "            # Save best model\n",
        "            if val_acc > best_val_acc:\n",
        "                best_val_acc = val_acc\n",
        "                best_model_state = deepcopy(model.state_dict())\n",
        "                torch.save(best_model_state, os.path.join(SAVE_DIR, f'best_model_fold_{fold+1}.pth'))\n",
        "                print(f'New best model saved with val acc: {best_val_acc:.2f}%')\n",
        "\n",
        "            # Early Stopping Check\n",
        "            if early_stopping(val_loss):\n",
        "                print(f'Early stopping triggered at epoch {epoch+1}')\n",
        "                break\n",
        "\n",
        "        # Save fold results\n",
        "        fold_results.append(max(history['val_acc']))\n",
        "\n",
        "        # Plot training curves\n",
        "        plt.figure(figsize=(12, 5))\n",
        "        plt.subplot(1, 2, 1)\n",
        "        plt.plot(history['train_loss'], label='Train Loss')\n",
        "        plt.plot(history['val_loss'], label='Val Loss')\n",
        "        plt.title('Loss Curve')\n",
        "        plt.legend()\n",
        "\n",
        "        plt.subplot(1, 2, 2)\n",
        "        plt.plot(history['train_acc'], label='Train Acc')\n",
        "        plt.plot(history['val_acc'], label='Val Acc')\n",
        "        plt.title('Accuracy Curve')\n",
        "        plt.legend()\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(f'fold_{fold+1}_curves.png')\n",
        "        plt.close()\n",
        "\n",
        "    # Final results\n",
        "    print(f'\\nK-Fold Results: {fold_results}')\n",
        "    print(f'Mean Accuracy: {np.mean(fold_results):.2f}% (Â±{np.std(fold_results):.2f})')\n",
        "\n",
        "def validate(model, val_loader):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in val_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs['cls'], labels)\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            _, predicted = outputs['cls'].max(1)\n",
        "            total += labels.size(0)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "    return total_loss/len(val_loader), 100.*correct/total\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    train_osp()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YiPleOf5srZz",
        "outputId": "8c387010-5fcc-4a9c-bdc5-7e4d2f696b57"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "\n",
            "Fold 1/5\n",
            "--------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pre-training Stage:\n",
            "Epoch 1/50 | Loss: 2.167 | Acc: 25.40% | Val Acc: 31.70%\n",
            "Epoch 2/50 | Loss: 1.602 | Acc: 40.05% | Val Acc: 38.00%\n",
            "\n",
            "Fine-tuning Stage:\n",
            "Epoch 1/100 | Loss: 1.414 | Acc: 54.73% | Val Acc: 45.00%\n",
            "New best model saved with val acc: 45.00%\n",
            "Epoch 2/100 | Loss: 1.330 | Acc: 58.05% | Val Acc: 47.50%\n",
            "New best model saved with val acc: 47.50%\n",
            "\n",
            "Fold 2/5\n",
            "--------------------\n",
            "Pre-training Stage:\n",
            "Epoch 1/50 | Loss: 2.089 | Acc: 27.02% | Val Acc: 30.90%\n",
            "Epoch 2/50 | Loss: 1.647 | Acc: 39.60% | Val Acc: 39.70%\n",
            "\n",
            "Fine-tuning Stage:\n",
            "Epoch 1/100 | Loss: 1.404 | Acc: 56.85% | Val Acc: 47.10%\n",
            "New best model saved with val acc: 47.10%\n",
            "Epoch 2/100 | Loss: 1.252 | Acc: 62.15% | Val Acc: 48.90%\n",
            "New best model saved with val acc: 48.90%\n",
            "\n",
            "Fold 3/5\n",
            "--------------------\n",
            "Pre-training Stage:\n",
            "Epoch 1/50 | Loss: 2.220 | Acc: 26.00% | Val Acc: 28.10%\n",
            "Epoch 2/50 | Loss: 1.649 | Acc: 40.50% | Val Acc: 35.10%\n",
            "\n",
            "Fine-tuning Stage:\n",
            "Epoch 1/100 | Loss: 1.405 | Acc: 55.70% | Val Acc: 45.80%\n",
            "New best model saved with val acc: 45.80%\n",
            "Epoch 2/100 | Loss: 1.282 | Acc: 59.58% | Val Acc: 47.40%\n",
            "New best model saved with val acc: 47.40%\n",
            "\n",
            "Fold 4/5\n",
            "--------------------\n",
            "Pre-training Stage:\n",
            "Epoch 1/50 | Loss: 2.285 | Acc: 23.30% | Val Acc: 25.60%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "M6XUfYAcV2iB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}